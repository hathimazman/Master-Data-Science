{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Tree Based Methods\n",
    "+ The prediction space are stratified or segmented into several regions.\n",
    "+ Mean or the mode of the training observations in the region to which it belongs usually being used for prediction.\n",
    "+ Decision trees are simple and easy to interpret.\n",
    "+ However, it's not as good as other supervised learning discussed previously.\n",
    "+ Bagging, random forests and boosting will produce mutiple trees which could improved the prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('images/pw81.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw82.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Trees\n",
    "+ Can be applied to both regression and classification. \n",
    "+ Decision trees are graphics where you can start at the \"root\" and traverse your way down by making decisions at the \"branches\" before finally ending up at a \"leaf\" that gives you the prediction. \n",
    "+ It usually drawn upside down (leaves at the bottom)\n",
    "\n",
    "![tree image](http://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-12-638.jpg?cb=1444928106)\n",
    "\n",
    "+ **terminal node** - the regions or leaves of the tree. (3 terminal nodes)\n",
    "+ **internal node** - the points along the tree where the predictor space is split. (2 internal nodes)\n",
    "+ **branches** - the segments of the trees that connect the nodes.\n",
    "+ Explanation for the example:\n",
    "    + *Years* of experienced is the most important factor in determining *Salary*.\n",
    "    + For experienced players, the number of *Hits* effect the *Salary*.\n",
    "    \n",
    "+ Two general steps for building a (regression) decision tree (Stratification)\n",
    "1. Divide the predictor space into j number of regions\n",
    "2. For each region, find the mean response and use it as the predicted value. This will minimize the squared error for that region.\n",
    "\n",
    "+ The regions in theory can be divided in any crazy manner you choose but in practice, they are divided into high dimensional rectangles. \n",
    "+ For example, you could have used a line with a non-zero slope to partition the region below to get a more accurate fit, but simplicity wins here and we just split on horizontal and vertical lines - \"high dimensional rectangles\".\n",
    "\n",
    "\n",
    "### How do we get the branches?\n",
    "+ We could try and just build every single tree imaginable and find the tree with the lowest squared error but this is computationally infeasible even for a relatively small number of predictors. \n",
    "+ Instead, a greedy approach is used by building the tree one branch at a time. \n",
    "+ The first branch is constructed by testing out many different binary splits of the data. \n",
    "\n",
    "+ For example, $X_1 < 5$ and  $X_1 >= 5$ would be one potential split. \n",
    "+ $X_2 = YES$ and  $X_2 = NO$ could be another binary split. \n",
    "+ Whichever split yields the lowest squared error would be considered the best split and that split would be chosen for the first branch. \n",
    "+ This process now continues for each branch interactively until some stopping criteria is met (maximum number of branches, minimum number of observations in a certain branch, etc...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw83.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Pruning\n",
    "+ It is possible to build a decision tree so specific (one with so many branches) that each observation can be predicted exactly. \n",
    "+ This would be complete memorization, ie overfitting, of the data. \n",
    "+ Because we want to have the tree work with unseen data, we can prune the tree.\n",
    "\n",
    "+ One strategy would be some have some threshold for stopping a branch from splitting - it must have decreased RSS by a certain amount. \n",
    "+ Since this might miss a good split deeper in the tree, pruning is preferred.\n",
    "\n",
    "Pruning works by: \n",
    "+ growing a very large tree, $T_0$, and prune it back to obtain a subtree.\n",
    "+ Goal is to select a subtree that leads to the lowest test error rate.\n",
    "+ Use CV to select the best subtree.\n",
    "\n",
    "Cost complexity pruning (weakest link pruning):\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when a minimum number of observations are left in each terminal node.\n",
    "2. At each stage during the growing process add a penalty term $\\alpha|T|$ to RSS where |T| is the number of terminal nodes and $\\alpha$ is the tuning parameter.\n",
    "3. This will give a function that maps $\\alpha$ to a particular subtree. So $\\alpha = 0$ would map to the original huge tree and for example $\\alpha = 5$ could map to a tree that with only half of the terminal nodes.\n",
    "4. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that\n",
    "$$ \\sum_{m=1}^{|T|}\\sum_{i: x_i \\in R_m}{(y_i - \\widehat{y}_{R_m})^2 + \\alpha |T|}$$\n",
    "is as small as possible.\n",
    "    + $|T|$ - no. of terminal nodes\n",
    "    + $R_m$ - rectangle corresponding to the $m$th terminal node\n",
    "\n",
    "Choose $\\alpha$ through cross validation by:\n",
    "1. Splitting training data into K folds\n",
    "2. Grow a large tree and apply the penalty term exactly as above (map each $\\alpha$ to a particular subtree.)\n",
    "3. evaluate each $\\alpha$ (subtree) on the left-out fold\n",
    "4. Average all the $\\alpha$ (subtrees) for each iteration of the K-folds\n",
    "\n",
    "Then use this $\\alpha$ to choose the tree from above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw84.png', width =600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw85.png', width =500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees\n",
    "+ Predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\n",
    "+ Interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.\n",
    "+ RSS cannot be used as a criterion for making the binary splits.\n",
    "\n",
    "#### How to determine splits\n",
    "+ Three different metrics can be used. \n",
    "+ First let $\\hat{p}_{mk}$ be the proportion of region m with class k. \n",
    "+ Remember that there can be many classes not just 2.\n",
    "1. Classification error rate: $$E = 1 - \\max_k(\\hat{p}_{mk})$$ \n",
    "where $\\max_k(\\hat{p}_{mk})$ is the most common class.\n",
    "2. Gini Index: $$G = \\sum\\limits_{k=1}^{K}\\hat{p}_{mk}(1 - \\hat{p}_{mk})$$\n",
    "Gini index is referred to as a measure of node purity â€” a small value indicates that a node contains predominantly\n",
    "observations from a single class.\n",
    "3. Cross Entropy: $$D = - \\sum\\limits_{k=1}^{K}\\hat{p}_{mk}\\log\\hat{p}_{mk}$$\n",
    "\n",
    "+ Cross entropy and Gini are similar in that it will yield low scores if all of the $\\hat{p}_{mk}$ close to 0 or 1. \n",
    "+ Gini and Cross entropy take into account each class proportion while classification error rate only takes into account the highest occurring class\n",
    "+ Both are more sensitive to node purity.\n",
    "+ Classification error rate is preferable if prediction accuracy is the goal.\n",
    "\n",
    "\n",
    "### Linear Model vs Trees\n",
    "+ If the decision boundary is linear and not rectangular then linear models can perform better than trees.\n",
    "+ But for highly non-linear decision boundaries then trees can perform better.\n",
    "+ Advantages of trees:\n",
    "    + Trees are very easy to explain\n",
    "    + more closely mirror human decision-making\n",
    "    + can be displayed graphically, therefore easily interpreted\n",
    "    + can easily handle qualitative predictors without the need to create dummy variables.\n",
    "+ Disadvantages of trees\n",
    "    + level of prediction accuracy are lower\n",
    "    + trees can be very non-robust. A small change in the data can cause a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw86.png', width =800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw87.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab\n",
    "Need to install graphviz to create the decision tree\n",
    "\n",
    "Download here:\n",
    "https://graphviz.gitlab.io/_pages/Download/Download_windows.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Classification Trees\n",
    "We'll start by using **classification trees** to analyze the `Carseats` data set. In these\n",
    "data, `Sales` is a continuous variable, and so we begin by converting it to a\n",
    "binary variable. We use the `ifelse()` function to create a variable, called\n",
    "`High`, which takes on a value of `Yes` if the `Sales` variable exceeds 8, and\n",
    "takes on a value of `No` otherwise. We'll append this onto our dataFrame using the `.map()` function, and then do a little data cleaning to tidy things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/carseats.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['High'] = df3.Sales.map(lambda x: 1 if x>8 else 0)\n",
    "df3.ShelveLoc = pd.factorize(df3.ShelveLoc)[0]\n",
    "df3.Urban = df3.Urban.map({'No':0, 'Yes':1})\n",
    "df3.US = df3.US.map({'No':0, 'Yes':1})\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly evaluate the performance of a classification tree on\n",
    "the data, we must estimate the test error rather than simply computing\n",
    "the training error. We first split the observations into a training set and a test\n",
    "set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3.drop(['Sales', 'High'], axis = 1)\n",
    "y = df3.High\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the DecisionTreeClassifier() function to fit a classification tree in order to predict High. Unfortunately, manual pruning is not implemented in sklearn: http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "However, we can limit the depth of a tree using the max_depth parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tree_carseats = DecisionTreeClassifier(max_depth = 6)\n",
    "classification_tree_carseats.fit(X_train, y_train)\n",
    "classification_tree_carseats.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training accuracy is 92.2%.\n",
    "\n",
    "One of the most attractive properties of trees is that they can be\n",
    "graphically displayed. Unfortunately, this is a bit of a roundabout process in `sklearn`. We use the `export_graphviz()` function to export the tree structure to a temporary `.dot` file,\n",
    "and the `graphviz.Source()` function to display the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(classification_tree_carseats, \n",
    "                out_file = \"carseat_tree.dot\", \n",
    "                feature_names = X_train.columns)\n",
    "\n",
    "with open(\"carseat_tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "check_call(['dot','-Tpng','carseat_tree.dot','-o','carseat_tree.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Price[X_train.Price<=92.5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ True(Left) or False(Right) based on the conditions in the previous node \n",
    "+ gini is the measure of impurity\n",
    "+ samples are the number of observations remaining to classify\n",
    "+ value is how many samples are in class 0(Low) and how many samples are in class 1(High)\n",
    "\n",
    "The most important indicator of High sales appears to be Price.\n",
    "\n",
    "Finally, let's evaluate the tree's performance on\n",
    "the test data. The `predict()` function can be used for this purpose. We can then build a confusion matrix, which shows that we are making correct predictions for\n",
    "around 72.5% of the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classification_tree_carseats.predict(X_test)\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, pred).T, \n",
    "                  index = ['No', 'Yes'], \n",
    "                  columns = ['No', 'Yes'])\n",
    "print(cm)\n",
    "# (36+22)/80 = 0.745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Regression Trees\n",
    "Now let's try fitting a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data using medv (median home value) as our response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = pd.read_csv('data/boston.csv')\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_df.drop('medv', axis = 1)\n",
    "y = boston_df.medv\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 0)\n",
    "\n",
    "# Pruning not supported. Choosing max depth 2)\n",
    "regr_tree_boston = DecisionTreeRegressor(max_depth = 2)\n",
    "regr_tree_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(regr_tree_boston, \n",
    "                out_file = \"boston_tree.dot\", \n",
    "                feature_names = X_train.columns)\n",
    "\n",
    "with open(\"boston_tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `lstat` measures the percentage of individuals with lower\n",
    "socioeconomic status. The tree indicates that lower values of `lstat` correspond\n",
    "to more expensive houses. The tree predicts a median house price\n",
    "of \\$45,766 for larger homes (`rm>=7.4351`) in suburbs in which residents have high socioeconomic\n",
    "status (`lstat<7.81`).\n",
    "\n",
    "Now let's see how it does on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = regr_tree_boston.predict(X_test)\n",
    "\n",
    "plt.scatter(pred, \n",
    "            y_test, \n",
    "            label = 'medv')\n",
    "\n",
    "plt.plot([0, 1], \n",
    "         [0, 1], \n",
    "         '--k', \n",
    "         transform = plt.gca().transAxes)\n",
    "\n",
    "plt.xlabel('pred')\n",
    "plt.ylabel('y_test')\n",
    "\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the regression tree is\n",
    "35.4. The square root of the MSE is therefore around 5.95, indicating\n",
    "that this model leads to test predictions that are within around \\$5,950 of\n",
    "the true median home value for the suburb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bagging\n",
    "+ Decison trees suffer from high variance.\n",
    "+ Bootstrap aggregation - a procedure for lowering variance of decision trees.\n",
    "    + Bootstrapping - taking repeated samples with replacement and using each sample as an input into a learning method  \n",
    "    + Aggregation - For Regression, average each prediction from each of these bootstrapped data sets. \n",
    "$$\\hat{f}_{\\textrm{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B{\\hat{f}^{*b}(x)}$$\n",
    "    + For classification, use majority vote, the most commonly occurring class among the B predictions.\n",
    "\n",
    "\n",
    "### Validation with Out-of-bag error\n",
    "+ Each bootstrap sample will contain on average 2/3 of the data points (because of sampling with replacement). \n",
    "+ We can use the left over 1/3 as validation set and record errors for each point not in the sample.\n",
    "+ The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.\n",
    "+ With B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.\n",
    "\n",
    "### Variable Importance\n",
    "+ The wonderful thing about decision trees is their ease of interpretability. \n",
    "+ Bagging reduces this tremendously since we are averaging decisions together. \n",
    "+ We can use the average amount of decrease in RSS/Gini-index that happens for each time a split happens with a particular predictor. \n",
    "+ Generally, those predictors that are used higher up in the trees will cause a more massive decrease in RSS/Gini-index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw88.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw89.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "+ Random Forests combine bagging with one additional feature - and that is to limit the features to split on at each node.\n",
    "+ Instead of being able to split the data on any feature, only a subset of the features are considered at each stage. \n",
    "+ A typical choice is the square root of the number of predictors, $m \\approx \\sqrt{p}$. \n",
    "+ This ensures that the trees will look quite different and not nearly as correlated as bagged trees would be. \n",
    "+ This also makes the trees significantly 'dumber' as they can only split on whatever random predictors it gets.\n",
    "+ This process decorrelate the trees, thereby making the average of the resulting trees less variable and hence more reliable.\n",
    "+ Refer Fig. 8.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw810.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Bagging and Random Forests\n",
    "\n",
    "Let's see if we can improve on this result using **bagging** and **random forests**. The exact results obtained in this section may\n",
    "depend on the version of `python` and the version of the `RandomForestRegressor` package\n",
    "installed on your computer, so don't stress out if you don't match up exactly with the book. Recall that **bagging** is simply a special case of\n",
    "a **random forest** with $m = p$. Therefore, the `RandomForestRegressor()` function can\n",
    "be used to perform both random forests and bagging. Let's start with bagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging: using all features\n",
    "bagged_boston = RandomForestRegressor(max_features = 13, random_state = 2)\n",
    "bagged_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `max_features = 13` indicates that all 13 predictors should be considered\n",
    "for each split of the tree -- in other words, that bagging should be done. How\n",
    "well does this bagged model perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bagged_boston.predict(X_test)\n",
    "\n",
    "plt.scatter(pred, \n",
    "            y_test, \n",
    "            label = 'medv')\n",
    "\n",
    "plt.plot([0, 1], \n",
    "         [0, 1], \n",
    "         '--k', \n",
    "         transform = plt.gca().transAxes)\n",
    "\n",
    "plt.xlabel('pred')\n",
    "plt.ylabel('y_test')\n",
    "\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the bagged regression tree is significantly lower than our single tree!\n",
    "\n",
    "We can grow a random forest in exactly the same way, except that\n",
    "we'll use a smaller value of the `max_features` argument. Here we'll\n",
    "use `max_features = 6`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests: using 6 features\n",
    "random_forest_boston = RandomForestRegressor(max_features = 4, random_state = 2)\n",
    "\n",
    "random_forest_boston.fit(X_train, y_train)\n",
    "\n",
    "pred = random_forest_boston.predict(X_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is even lower; this indicates that random forests yielded an\n",
    "improvement over bagging in this case.\n",
    "\n",
    "Using the `feature_importances_` attribute of the `RandomForestRegressor`, we can view the importance of each\n",
    "variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_boston.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importance = pd.DataFrame({'Importance':random_forest_boston.feature_importances_*100}, \n",
    "                          index = X.columns)\n",
    "\n",
    "Importance.sort_values(by = 'Importance', \n",
    "                       axis = 0, \n",
    "                       ascending = True).plot(kind = 'barh', \n",
    "                                              color = 'r', )\n",
    "\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that across all of the trees considered in the random\n",
    "forest, the wealth level of the community (`lstat`) and the house size (`rm`)\n",
    "are by far the two most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import _tree\n",
    "\n",
    "def leaf_depths(tree, node_id = 0):\n",
    "    \n",
    "     '''\n",
    "     tree.children_left and tree.children_right store ids\n",
    "     of left and right chidren of a given node\n",
    "     '''\n",
    "     left_child = tree.children_left[node_id]\n",
    "     right_child = tree.children_right[node_id]\n",
    "\n",
    "     '''\n",
    "     If a given node is terminal, \n",
    "     both left and right children are set to _tree.TREE_LEAF\n",
    "     '''\n",
    "     if left_child == _tree.TREE_LEAF:\n",
    "         \n",
    "         '''\n",
    "         Set depth of terminal nodes to 0\n",
    "         '''\n",
    "         depths = np.array([0])\n",
    "\n",
    "     else:\n",
    "         \n",
    "         '''\n",
    "         Get depths of left and right children and\n",
    "         increment them by 1\n",
    "         '''\n",
    "         left_depths = leaf_depths(tree, left_child) + 1\n",
    "         right_depths = leaf_depths(tree, right_child) + 1\n",
    " \n",
    "         depths = np.append(left_depths, right_depths)\n",
    " \n",
    "     return depths\n",
    "\n",
    "def leaf_samples(tree, node_id = 0):\n",
    "    \n",
    "     left_child = tree.children_left[node_id]\n",
    "     right_child = tree.children_right[node_id]\n",
    "\n",
    "     if left_child == _tree.TREE_LEAF:\n",
    "        \n",
    "         samples = np.array([tree.n_node_samples[node_id]])\n",
    "\n",
    "     else:\n",
    "        \n",
    "         left_samples = leaf_samples(tree, left_child)\n",
    "         right_samples = leaf_samples(tree, right_child)\n",
    "\n",
    "         samples = np.append(left_samples, right_samples)\n",
    "\n",
    "     return samples\n",
    "\n",
    "def draw_tree(ensemble, tree_id=0):\n",
    "\n",
    "     plt.figure(figsize=(8,8))\n",
    "     plt.subplot(211)\n",
    "\n",
    "     tree = ensemble.estimators_[tree_id].tree_\n",
    "\n",
    "     depths = leaf_depths(tree)\n",
    "     plt.hist(depths, histtype='step', color='#9933ff', \n",
    "              bins=range(min(depths), max(depths)+1))\n",
    "\n",
    "     plt.xlabel(\"Depth of leaf nodes (tree %s)\" % tree_id)\n",
    "    \n",
    "     plt.subplot(212)\n",
    "    \n",
    "     samples = leaf_samples(tree)\n",
    "     plt.hist(samples, histtype='step', color='#3399ff', \n",
    "              bins=range(min(samples), max(samples)+1))\n",
    "    \n",
    "     plt.xlabel(\"Number of samples in leaf nodes (tree %s)\" % tree_id)\n",
    "   \n",
    "     plt.show()\n",
    "\n",
    "draw_tree(random_forest_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ensemble(ensemble):\n",
    "\n",
    "     plt.figure(figsize=(8,8))\n",
    "     plt.subplot(211)\n",
    "\n",
    "     depths_all = np.array([], dtype=int)\n",
    "\n",
    "     for x in ensemble.estimators_:\n",
    "         tree = x.tree_\n",
    "         depths = leaf_depths(tree)\n",
    "         depths_all = np.append(depths_all, depths)\n",
    "         plt.hist(depths, histtype='step', color='#ddaaff', \n",
    "                  bins=range(min(depths), max(depths)+1))\n",
    "\n",
    "     plt.hist(depths_all, histtype='step', color='#9933ff', \n",
    "              bins=range(min(depths_all), max(depths_all)+1), \n",
    "              weights=np.ones(len(depths_all))/len(ensemble.estimators_), \n",
    "              linewidth=2)\n",
    "     plt.xlabel(\"Depth of leaf nodes\")\n",
    "    \n",
    "     samples_all = np.array([], dtype=int)\n",
    "    \n",
    "     plt.subplot(212)\n",
    "    \n",
    "     for x in ensemble.estimators_:\n",
    "         tree = x.tree_\n",
    "         samples = leaf_samples(tree)\n",
    "         samples_all = np.append(samples_all, samples)\n",
    "         plt.hist(samples, histtype='step', color='#aaddff', \n",
    "                  bins=range(min(samples), max(samples)+1))\n",
    "    \n",
    "     plt.hist(samples_all, histtype='step', color='#3399ff', \n",
    "              bins=range(min(samples_all), max(samples_all)+1), \n",
    "              weights=np.ones(len(samples_all))/len(ensemble.estimators_), \n",
    "              linewidth=2)\n",
    "     plt.xlabel(\"Number of samples in leaf nodes\")\n",
    "    \n",
    "     plt.show()\n",
    "        \n",
    "draw_ensemble(random_forest_boston)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Boosting\n",
    "+ Boosting can be used (like bagging) for many different learning algorithms. \n",
    "+ Trees are grown sequentially. Each tree is grown using information from a previous tree. \n",
    "+ Boosting learns slowly. A tree is fit to the *residuals* of the model and not the outcome Y. \n",
    "+ Each tree can be small with just a few terminal nodes. \n",
    "+ The new trees keep focusing on areas where the model performs poorly. \n",
    "+ For classification problems the log-odds are used to find the residuals.\n",
    "\n",
    "Parameters to model, \n",
    "* B - the number of trees to fit\n",
    "* d - the max number of terminal nodes for each tree\n",
    "* $\\lambda$ - learning (shrinkage) parameter that determines how much weight to assign to each tree. It control the speed of the process. (0.01 or 0.001)\n",
    "\n",
    "Add up all trees (times $\\lambda$) together to get final model.\n",
    "$$\\hat{f}(x) = \\sum_{b=1}^B{\\lambda \\hat{f}^b(x)}$$\n",
    "\n",
    "### Ensemble of weak learners\n",
    "Random forest and Boosted trees are two excellent algorithms that both use 'weak' learners. On their own each learner is quite terrible but when combined together with many, make them very powerful, more powerful than an expert in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw811.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Boosting\n",
    "\n",
    "Now we'll use the `GradientBoostingRegressor` package to fit **boosted\n",
    "regression trees** to the `Boston` data set. The\n",
    "argument `n_estimators = 500` indicates that we want 500 trees, and the option\n",
    "`interaction.depth = 4` limits the depth of each tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_boston = GradientBoostingRegressor(n_estimators = 500, \n",
    "                                           learning_rate = 0.01, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 1)\n",
    "\n",
    "boosted_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check out the feature importances again:\n",
    "\n",
    "feature_importance = boosted_boston.feature_importances_*100\n",
    "\n",
    "rel_imp = pd.Series(feature_importance, \n",
    "                    index = X.columns).sort_values(inplace = False)\n",
    "\n",
    "rel_imp.T.plot(kind = 'barh', \n",
    "               color = 'r', )\n",
    "\n",
    "plt.xlabel('Variable Importance')\n",
    "\n",
    "plt.gca().legend_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `lstat` and `rm` are again the most important variables by far. Now let's use the boosted model to predict `medv` on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, boosted_boston.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE obtained is similar to the test MSE for random forests\n",
    "and superior to that for bagging. If we want to, we can perform boosting\n",
    "with a different value of the shrinkage parameter $\\lambda$. Here we take $\\lambda = 0.2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_boston2 = GradientBoostingRegressor(n_estimators = 500, \n",
    "                                            learning_rate = 0.2, \n",
    "                                            max_depth = 4, \n",
    "                                            random_state = 1)\n",
    "boosted_boston2.fit(X_train, y_train)\n",
    "\n",
    "mean_squared_error(y_test, boosted_boston2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using $\\lambda = 0.2$ leads to a slightly lower test MSE than $\\lambda = 0.01$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leo Breiman - Random Forest Creator\n",
    "Excellent site and a fairly easy read: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "\n",
    "# Extreme Gradient Boosting (XGBoost)\n",
    "Winner of many kaggle competitions: See explanation of algorithm here: http://xgboost.readthedocs.io/en/latest/model.html#\n",
    "\n",
    "# Extremely Random Trees\n",
    "Random forests where only a random subset of the cutpoints are allowed. Choose the best cutpoint from that random set. http://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. statistician or data scientist\n",
    "\n",
    "            do you like measure theory\n",
    "           /                          \\\n",
    "          yes                          no\n",
    "          /                             \\\n",
    "        statistician                   would you like to code for more than 4 hours each day\n",
    "                                       /            \\\n",
    "                                     yes            no\n",
    "                                     /               \\\n",
    "       taken more than 5 machine learning courses?   statistician  \n",
    "       /                              \\\n",
    "       yes                           no\n",
    "       /                              \\\n",
    "       do you like big data?           are you a frequentist?\n",
    "       /              \\                       /          \\\n",
    "       yes            no                     yes          no\n",
    "       |               |                      |            |\n",
    "       data scientist  stats              statistician   data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "A sum of one depth trees are basis functions with indicator variables and a sum of basis functions is an additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "p1 = np.linspace(0.001, .9999, 1000)\n",
    "p2 = 1 - p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = 1 - np.column_stack((p1, p2)).max(axis=1)\n",
    "gini = p1 * (1 - p1) + p2 * (1 - p2)\n",
    "entropy = -(p1 * np.log(p1) + p2 * np.log(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p1, classification_error)\n",
    "plt.plot(p1, gini)\n",
    "plt.plot(p1, entropy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a\n",
    "      x1 > 1\n",
    "       /  \\\n",
    "     x2 > 1  5\n",
    "     /     \\  \n",
    "    x1 < 0  15 \n",
    "    /      \\\n",
    "    x2>.5   3\n",
    "    /   \\\n",
    "    0   10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(-1, 2)\n",
    "plt.ylim(0, 3)\n",
    "plt.hlines(1, -1, 2)\n",
    "plt.vlines(1, 0, 1)\n",
    "plt.hlines(2, -1, 2)\n",
    "plt.vlines(0, 2, 3)\n",
    "plt.text(-.3, .4, \"-1.8\", fontsize=30)\n",
    "plt.text(1.3, .4, \".63\", fontsize=30)\n",
    "plt.text(.3, 1.4, \"2.49\", fontsize=30)\n",
    "plt.text(-.8, 2.4, \"-1.06\", fontsize=30)\n",
    "plt.text(.7, 2.4, \".21\", fontsize=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5\n",
    " Majority vote: Choose Red     \n",
    " Non-red: 4 votes  \n",
    " red: 6 votes  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average probability\n",
    "choose green (.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average Probability Choose green\n",
    "np.mean([.1, .15, .2, .2, .55, .6, .6,.65, .7, .75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "For a regression tree, look through each feature and for each feature split the feature in two parts for each unique value of that feature and calcaulate RSS. Choose first split with lowest RSS. \n",
    "\n",
    "Now you have two nodes, repeat procedure until only a set number of observations are left in each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston = pd.read_csv('data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.iloc[:, :-1]\n",
    "y = boston['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = range(1, X.shape[1] + 1)\n",
    "num_trees = [5, 10, 20, 30, 50, 70, 100, 150, 200, 250, 300, 400, 500, 600, 700, 800, 900, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mf in max_features:\n",
    "    for nm in num_trees:\n",
    "        clf = RandomForestRegressor(oob_score=True, max_features=mf, n_estimators=nm)\n",
    "        clf.fit(X, y)\n",
    "        max_features_dict[mf].append(clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "for mf, values in max_features_dict.items():\n",
    "    plt.plot(num_trees[:5], values[:5], label=mf)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "for mf, values in max_features_dict.items():\n",
    "    plt.plot(num_trees[6:10], values[6:10], label=mf)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "for mf, values in max_features_dict.items():\n",
    "    plt.plot(num_trees[11:], values[11:], label=mf)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Not much difference in R-squared after 20 trees and at least 3 predictor variables used at each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('data/carseats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.get_dummies(carseats, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = carseats.iloc[:, 1:]\n",
    "y = carseats['Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "dot_data = StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(clf, out_file=dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph[0].write_png(\"images/carseats_tree.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/carseats_tree.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
