{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Used to predict quantitative (numerical) response.\n",
    "+ Can we determine if a relationship exists between predictors and response?\n",
    "+ How strong is this relationship?\n",
    "+ Which predictors are related to response?\n",
    "+ How accurate are predictions once model is fit?\n",
    "+ Is a linear model appropriate?\n",
    "+ Are there interaction effects?\n",
    "\n",
    "## Simple Linear Regression (SLR)\n",
    "Technically this means one predictor is linearly related to the response.\n",
    "\t$$ Y = \\beta_0 +  \\beta_1 X + \\epsilon$$\n",
    "\n",
    "There are two unknown constants that we need to estimate, the intercept $\\beta_0$ and the slope, $\\beta_1$. Also called coefficients or parameters. \n",
    "\t\n",
    "Once they are estimated, the estimated fit becomes \n",
    "\t$$ \\hat{Y} = \\hat{\\beta_0} +  \\hat{\\beta_1} X$$\n",
    "\n",
    "There are various ways of estimating the coefficients. The most common approach is via the least squares technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv = pd.read_csv('data/Advertising.csv')\n",
    "adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(adv['TV'], adv['Sales'], deg=1)\n",
    "y_hat = fit[1] + adv['TV'] * fit[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x='TV', y='Sales', data=adv)\n",
    "plt.vlines(adv['TV'], y_hat, adv['Sales'], lw = .4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the Accuracy of the Coefficient Estimates\n",
    "The theoretical best linear relationship can be defined as: \n",
    "\t$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n",
    "\n",
    "This **population regression line** will never be known in practice and remain unobserved unless it came from simulated data.\n",
    "\n",
    "**Unbiased** - An estimator that doesn't systematically over or underestimate the value of the parameter it is estimating\t\n",
    "\n",
    "**How much will the linear regression line expect to vary from sample to sample?**\n",
    "\n",
    "In the case of estimating a sample mean $\\hat{\\mu}$ from a number of points $n$, we get that the $Var(\\hat{\\mu}) = \\frac{\\sigma^2}{n}$ where $\\sigma$ is the standard distribution of the original set of $n$ points. \n",
    "\n",
    "The square root of this value is called the **standard error** and gives us a rough idea of how much the estimator will change from sample to sample.\n",
    "The standard errors of $\\beta_0$ and $\\beta_1$ are\n",
    "\t$$\tSE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n{\\left(x_i - \\bar{x} \\right)^2}} \\right] $$\n",
    "\t$$\tSE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n{\\left(x_i - \\bar{x} \\right)^2}} $$\n",
    "    \n",
    "Knowing how to derive our standard error and assuming the errors are Gaussian we can generate a **confidence interval** based on a $t$-distribution. \n",
    "For instance, approximately 95\\% of all samples will be contained in the following interval: $$\\beta_1 \\pm 1.96 \\cdot SE(\\beta_1)$$\n",
    "\n",
    "If the standard error is large and the estimated value small then the estimator might not be significantly different than 0, meaning it statistically is insignificant. To test significance a hypothesis test can be done on any of the predictors. The hypothesis test is usually done to test whether the predictor is different than 0. \n",
    "\n",
    "The null hypothesis \n",
    "\t$$H_0: \\beta_1 = 0 $$\n",
    "is tested against the alternative\n",
    "$$H_a: \\beta_1 \\ne 0 $$\n",
    "\n",
    "To test this we find out how many standard errors our parameter is away from 0. \n",
    "\t$$t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})}$$\n",
    "\n",
    "The $t$-distribution is then used to determine the percentage of time that a random value would fall beyond this t-statistic. This percentage is called the **$p$-value** and used as a measure of how extreme the results of your data are. \n",
    "The smaller the $p$-value the more extreme the results and the more likely they did not come from randomness.\n",
    "\n",
    "### Using the statsmodels api\n",
    "The excellent [statsmodels library](http://statsmodels.sourceforge.net/) integrates neatly with the pandas DataFrame to do statistical analysis. Some simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = smf.ols('Sales ~ TV', data=adv).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression with newspaper\n",
    "results2 = smf.ols('Sales ~ Newspaper', data=adv).fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = smf.ols('Sales ~ Radio', data=adv).fit()\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "Instead of running a simple linear model for each predictor, a model can be built that incorporates all of the predictors. \n",
    "\n",
    "The MLR model is\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "A few important questions in MLR:\n",
    "1. Is at least one of the predictors $X_1,X_2, \\ldots, X_p$ useful in predicting the response?\n",
    "2. Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data? **Use R$^2$ or RSE**\n",
    "\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction? **Fit into the model. Use PI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1: Test the hypothesis \n",
    "$$H_0 : \\beta_0 = \\beta_1 = \\beta_2 = \\cdots = \\beta_p =0$$\n",
    "vs\n",
    "\n",
    "$$H_a : \\textrm{at least one } \\beta_j \\neq 0$$\n",
    "\n",
    "An F test is performed to test for significance.\n",
    "$$ F = \\frac{(SST - SSE)/p}{SSE / (n - p - 1)}$$ \n",
    "\n",
    "or just use the P-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = smf.ols('Sales ~ TV + Newspaper + Radio', data=adv).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newspaper is least highly correlated with Sales\n",
    "# Its relatively high correlation to Radio could be the reason it was significant on its own and not\n",
    "# when Radio was also in the model\n",
    "adv.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Sales ~ TV + Radio', data=adv).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2: Selecting a subset of a model\n",
    "In the above model, Newspaper does not appear to have a relationship with sales and hence would be a good candidate to drop from our model. But if the number of predictors were more, it might be troubling to manually fit many models and hand-select which variables to include in the model. Forward, backward and mixed selection processes can be used to find a better model. All of these selection models make their variable selection based on some statistic - AIC, BIC, Mallows CP, Adjusted R-squared\n",
    "\n",
    "* Forward - starts with an empty model and adds one variable at a time until the statistic is maximized\n",
    "* Backward - starts with a full model and removes one variable at a time\n",
    "* Mixed - starts empty and either removes or adds a variable at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical predictor variables\n",
    "Variables that are non-numeric or are numerical but represent categories are called categorical variables. Also called qualitative or factor variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('data/Credit.csv')\n",
    "credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['Female'] = (credit.Gender == 'Female').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Balance ~ Female', data=credit).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing\n",
    "credit[credit['Female'] == 0]['Balance'].mean(), credit[credit['Female'] == 1]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Balance ~ Female + Age + Income', data=credit).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The broken assumptions of a linear model\n",
    "There are several assumptions that are used when fitting a linear model. \n",
    "* The errors are normally distributed and have constant variance\n",
    "* The errors are not correlated with one another\n",
    "* The predictor variables are independent. An increase in one won't result in an increase in another\n",
    "* The change in response for a one unit increase in X is the same no matter what the value of X\n",
    "\n",
    "# Challenging the linearity constraint through interaction effects\n",
    "In a linear regression with no interaction effects (no two predictors are multiplied together) and the assumption is that an increase in one unit in one variable will not have any effect on another variable. In many real world problems an increase in one variable might change the impact that another variable has on the response. To capture this in multiple regression, we multiply the predictors together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction model\n",
    "results = smf.ols('Sales ~ TV + Radio + TV * Radio', data=adv).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression is Still Linear\n",
    "Despite the fact that the regression line can be visibly non-linear the squaring predictor variables still means we are doing linear regression. The requirements for a regression to be 'linear' is to have it linear in the parameters. Heres a good link discussing the difference between linear and non-linear regression. http://blog.minitab.com/blog/adventures-in-statistics/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonlinearity of data\n",
    "resid = adv['Sales'] - results.predict(adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like a non-random shape. data appears slightly non-linear though not too bad\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(results.predict(adv), resid)\n",
    "plt.ylim(-2, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "In the book, the lab focuses on performing a linear regression on the Boston dataset. We will do so using seaborn, statsmodels and scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('data/boston.csv')\n",
    "boston.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='lstat', y='medv', data=boston);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels\n",
    "results = smf.ols('medv ~ lstat', data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at residuals\n",
    "# Yikes. lots of nonlinearity. Need a different model\n",
    "plt.scatter(results.fittedvalues, results.resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns. No easy way to do this like in R\n",
    "# Mostly highly significant variables\n",
    "string_cols = ' + '.join(boston.columns[:-1])\n",
    "results = smf.ols('medv ~ {}'.format(string_cols), data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove age\n",
    "string_cols = ' + '.join(boston.columns[:-1].difference(['age']))\n",
    "results = smf.ols('medv ~ {}'.format(string_cols), data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction\n",
    "results = smf.ols('medv ~ lstat * age', data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction\n",
    "results = smf.ols('medv ~ lstat + np.power(lstat, 2)', data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = smf.ols('medv ~ lstat', data=boston).fit()\n",
    "results2 = smf.ols('medv ~ lstat + np.power(lstat, 2)', data=boston).fit()\n",
    "\n",
    "anova_lm(results1, results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_lm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually compute F\n",
    "(results1.ssr - results2.ssr) / (results2.ssr / results2.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_lm(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('data/carseats.csv')\n",
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Sales ~ ShelveLoc + Price + Urban', data=carseats).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "There are 3 different null hypotheses for each of TV, Radio and Newspaper each testing whether there is a relationship from that variable to Sales given that the other two variables are held constant. From this model we can reject the null hypotheses that both TV and Radio have no correspondence with sales. We fail to reject the null hypotheses that Newspaper advertising is related to Sales.\n",
    "\n",
    "# 2\n",
    "KNN classification predicts as the category who has the highest frequency among it's k nearest neighbors. KNN regression predicts the mean of its nearest K neighbors.\n",
    "\n",
    "# 3\n",
    "a) iii is correct. Males will earn more than females GPA is high enough. Higher than 3.5 to be exact to wipe away the female advantage.  \n",
    "b) 50 + (20 * 4) + (.07 * 110) + (35 * 1) + (.01 * 110 * 4)  - (10 * 1 * 4) = 137.1  \n",
    "c) False, it all comes down to the standard error of the coefficient to determine significance. It could very well be the most significant factor.\n",
    "\n",
    "# 4\n",
    "a) For training data, the RSS always decreases as model complexity increases so the cubic model will have lower RSS.  \n",
    "b) For test data, the RSS for the linear model should do better as the cubic model will have fit noise and the true model is linear.  \n",
    "c) Cubic model. Same answer as a)  \n",
    "d) This would be impossible to know. It could go both ways as the true model is not known. Must compute RSS on test data in this case\n",
    "\n",
    "# 5\n",
    "Combining the first equation $\\hat{y_i} = x_i\\hat{\\beta}$ with (3.38) we get $$\\hat{y_i} = \\frac{x_i\\sum\\limits_{k=1}^n x_k y_k}{\\sum\\limits_{j=1}^nx_j^2}$$\n",
    "\n",
    "The $x_i$ outside of the summation is a constant and be distributed inside the summation. $$\\hat{y_i} = \\sum\\limits_{k=1}^n (\\frac{x_i x_k}{\\sum\\limits_{j=1}^nx_j^2})y_k$$\n",
    "\n",
    "$a_i$ is everything between the parentheses. $$a_i = \\frac{x_i x_k}{\\sum\\limits_{j=1}^nx_j^2}$$\n",
    "\n",
    "# 6\n",
    "Just rearrange the second equation in 3.4 and you have the equality \n",
    "# 7\n",
    "See image below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/Chapter 3 - 7 proof.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('data/auto.csv')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('mpg ~ horsepower', data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Yes there is strong evidence of a relationship between mpg and horsepower  \n",
    "ii) Just from the summary it is very strong as the t-statistic is -24 though there is still lots of variation left in the model with an r-squared of .6  \n",
    "iii) negative  \n",
    "iv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params['Intercept'] + results.params['horsepower'] * 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, data, ss2 = summary_table(results, alpha=0.05)\n",
    "\n",
    "fittedvalues = data[:,2]\n",
    "predict_mean_se  = data[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T\n",
    "predict_ci_low, predict_ci_upp = data[:,6:8].T\n",
    "\n",
    "# wls cinv\n",
    "prstd, iv_l, iv_u = wls_prediction_std(results)\n",
    "\n",
    "# plot OLS\n",
    "cil, = plt.plot(auto['horsepower'], predict_ci_low, 'r--', lw=1, alpha=0.5)\n",
    "ciu, = plt.plot(auto['horsepower'], predict_ci_upp, 'r--', lw=1, alpha=0.5)\n",
    "mcil, = plt.plot(auto['horsepower'], predict_mean_ci_low, 'b--', lw=1, alpha=0.5)\n",
    "mciu, = plt.plot(auto['horsepower'], predict_mean_ci_upp, 'b--', lw=1, alpha=0.5)\n",
    "\n",
    "\n",
    "plt.scatter(auto['horsepower'], auto['mpg']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compute confidence or prediction interval given an x value\n",
    "def create_interval(ols_result, interval_type, alpha, x_values, conf_x):\n",
    "    if interval_type == 'confidence':\n",
    "        add_one = 0\n",
    "    elif interval_type == 'prediction':\n",
    "        add_one = 1\n",
    "    else:\n",
    "        print(\"Choose interval_type as confidence or prediction\")\n",
    "        return\n",
    "    n = len(x_values)\n",
    "    t_value = stats.t.ppf(1 - alpha / 2, df = n - 2)\n",
    "    sy = np.sqrt((ols_result.resid ** 2).sum() / (n - 2))\n",
    "    numerator = (conf_x - x_values.mean()) ** 2\n",
    "    denominator = ((x_values - x_values.mean()) ** 2).sum()\n",
    "    interval = t_value * sy * np.sqrt(add_one + 1 / n + numerator / denominator)\n",
    "    prediction = results.params[0] + results.params[1] * conf_x\n",
    "    return (prediction - interval, prediction + interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interval(results, 'confidence', .05, auto['horsepower'], 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interval(results, 'prediction', .05, auto['horsepower'], 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severe problems with the data\n",
    "plt.scatter(results.fittedvalues, results.resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ ' + \" + \".join(auto.columns[1:-1])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols(formula, data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) There is a clear relationship between predictor and response. F-stat is very high.  \n",
    "ii) displacement, weight, year, origin are statistically significant    \n",
    "iii) Its positive, so the higher the year the more the mpg    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) look at diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_influence = OLSInfluence(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# looks very similar to previous problem\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,10))\n",
    "ax[0, 0].scatter(results.fittedvalues, results.resid)\n",
    "ax[0, 0].set_ylabel(\"Raw Residuals\")\n",
    "ax[1, 0].scatter(results.fittedvalues, results_influence.resid_studentized_external)\n",
    "ax[1, 0].set_ylabel(\"Studentized Residual\")\n",
    "sm.graphics.qqplot(results.resid / np.sqrt((results.resid ** 2).sum() / 390), line='45', ax=ax[0, 1])\n",
    "ax[1, 1].scatter(results_influence.resid_studentized_external ** 2, results_influence.influence);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most residuals fall within 3 standard deviations and the qqplot looks relatively good until the right tail where a few observations are above 3 standard deviations indicating outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point 13 has unusually large leverage\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Add interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ ' + \" + \".join(auto.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_formula =  \" + \".join([comb[0] + \" * \" + comb[1] for comb in combinations(auto.columns[1:-1], 2)])\n",
    "interactions_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ ' + \" + \".join(auto.columns[1:-1])\n",
    "formula += ' + ' + interactions_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols(formula, data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding all possible (7c2 = 21) interaction combination effects to the model only one of them is significant at the .01 level. Acceleration * origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add displacement squared to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula += ' + np.power(displacement, 2)'\n",
    "results = smf.ols(formula, data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of multicolinearity going on here\n",
    "results = smf.ols('mpg ~ displacement + origin + np.power(displacement, 2)', data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqrt of horsepower has higher r-squared than horsepower by itself\n",
    "results = smf.ols('mpg ~ np.sqrt(horsepower)', data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
    "ax1.scatter(auto['horsepower'], auto['mpg'])\n",
    "ax1.set_title(\"Horsepower vs MPG\")\n",
    "ax2.scatter(np.log(np.log(auto['horsepower'])), auto['mpg'])\n",
    "ax2.set_title(\"Log(Log(Horsepower)) vs MPG\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared increases a bit more with log-log-horsepower\n",
    "results = smf.ols('mpg ~ np.log(np.log((horsepower)))', data=auto).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Carseats data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('data/carseats.csv')\n",
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Sales ~ Price + Urban + US', data=carseats).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only US and Price are statistically significant in our model. There is no difference whether someone is living in an urban area or not. Living in the US adds 1.2 to Sales up from 13 for outside of US. For every 1 unit increase in Price a corresponding .05 decrease in sales is seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# equations\n",
    "Ignoring Urban because its not significant.\n",
    "* In US: $Sales = 14.24 - .055 * Price$\n",
    "* Not in US: $Sales = 13.04 - .055 * Price$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Reject null for US and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) smaller model without urban\n",
    "results = smf.ols('Sales ~ Price + US', data=carseats).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Since urban is nearly completely random, there is almost no difference in the two models above. R-squared is low so lots of variance remains in the model  \n",
    "g) See table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't appear to be outliers\n",
    "plt.scatter(results.fittedvalues, results.resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few high leverage points above .025\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(100)\n",
    "y = 2 * x + np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No constant. Highly signifcant predictor\n",
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x onto y. Same as above\n",
    "results = sm.OLS(x, y).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) The derived equation is symmetric to x and y, meaning you can replace x and y and the equation would be the exact same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.OLS(x, sm.add_constant(y)).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-statistic and t-stat is same with an intercept\n",
    "results = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12\n",
    "a) Using equation 3.38, the coefficients will be the same when $\\sum{y^2} = \\sum{x^2}$  \n",
    "b) Its very difficult to get the exact same coefficients. Any random pairing will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(100)\n",
    "y = x + np.random.randn(100) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very close to a perfect line\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients are just a little different\n",
    "results = sm.OLS(x, y).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) if x and y are the exact same (but in a different order) the coefficients for the model should be the same\n",
    "x = np.random.randn(100) * 5\n",
    "y = x.copy()\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.OLS(x, y).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same coefficient!\n",
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "eps = np.random.randn(100) * .25\n",
    "y = -1 + .5 * x + eps # b0 = -1 and b1 = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient estimates are very close to actual\n",
    "results = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they are very close to one another\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, -1 + .5 * x, label ='pop')\n",
    "plt.plot(x, results.params[0] + results.params[1] * x, label = 'fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x squared is not significant\n",
    "x2 = np.column_stack((np.ones(100), x, x ** 2))\n",
    "results = sm.OLS(y, x2).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confidence intervals will shrink/expand with eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x1 = np.random.rand(100)\n",
    "x2 = .5 * x1 + np.random.rand(100) / 10\n",
    "y = 2 + 2 * x1 + .3 * x2 + np.random.randn(100)\n",
    "# regresion coeffs are 2, 2, .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very highly correlated. Only differ by random factor between 0 and .1\n",
    "np.corrcoef(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1, x2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprisingly both variables are not significant\n",
    "X = np.column_stack((np.ones(100), x1, x2))\n",
    "results = sm.OLS(y, X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones(100), x1))\n",
    "results = sm.OLS(y, X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones(100), x2))\n",
    "results = sm.OLS(y, X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since x1 and x2 are very highly correlated to one another it makes sense that when substituted in a linear model for one another a very similar r-squared would be achieved. The high collinearity is causing havoc with the model when both x1 and x2 are in the model. We know beforehand that each variable has a positive relationship with y and in the first model, x1 is positive and x2 is negative which is an impossibility. The standard errors for each predictor grow because of the collinearity which causes them not to be significant when they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_new = np.append(x1, .1)\n",
    "x2_new = np.append(x2, .8)\n",
    "y_new = np.append(y, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((y_new, x1_new, x2_new))\n",
    "df_new = pd.DataFrame(X, columns=['y', 'x1', 'x2'])\n",
    "results = smf.ols('y ~ x2', data=df_new).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation goes way down with one point\n",
    "np.corrcoef(x1_new, x2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge outlier here\n",
    "plt.scatter(x1_new, x2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not an outlier in terms of residual, but very likely very influential\n",
    "plt.scatter(results.fittedvalues, results.resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup its unbelievably influential\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) simple linear regression for each predictor\n",
    "boston = pd.read_csv('data/boston.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below prints the confindence interval for each predictor in a simple linear regression\n",
    "# Nearly all the predictors have 95% confindence bands that don't include 0 meaning they rejecy the null hypothesis\n",
    "# the only predictors that fails to reject null: chas \n",
    "for col in boston.columns[1:]:\n",
    "    results = smf.ols('crim ~ {}'.format(col), data=boston).fit()\n",
    "    print(results.conf_int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'crim ~ ' + ' + '.join(boston.columns[1:])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variables in model. Many are not significant now\n",
    "results = smf.ols(formula, data=boston).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all significant predictors\n",
    "results.tvalues[abs(results.tvalues) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all coefficients from multiple reression model\n",
    "multiple_linear_params = results.params.iloc[1:]\n",
    "\n",
    "simple_linear_params = pd.Series()\n",
    "for col in boston.columns[1:]:\n",
    "    results_slr = smf.ols('crim ~ {}'.format(col), data=boston).fit()\n",
    "    simple_linear_params = simple_linear_params.append(results_slr.params.loc[[col]])\n",
    "    \n",
    "both_models = pd.DataFrame({'simple': simple_linear_params, 'multiple':multiple_linear_params})\n",
    "both_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_models.plot.scatter('simple', 'multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are several variables raised to the power of 2 or 3 that are showing significance\n",
    "# but these need to be combined with other variables to assess their actual importance\n",
    "for col in boston.columns[1:]:\n",
    "    results = smf.ols('crim ~ {} + np.power({}, 2) + np.power({}, 3)'.format(col, col, col), data=boston).fit()\n",
    "    print(results.conf_int())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
