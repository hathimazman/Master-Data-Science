for (i in length(data$text)) {
# Sentiment analysis
sentiment <- get_sentiment(text)
newdf$sentiment[i] <- sentiment$ave_sentiment
}
get_sentiment(newdf[1,1])
for (i in length(data$text)) {
# Sentiment analysis
sentiment <- get_sentiment(text)
newdf$sentiment[i] <- sentiment
}
newdf
newdf <- data.frame(
text = data$text,
sentiment = rep(1.1, nrow(data)))
for (i in length(data$text)) {
# Sentiment analysis
sentiment <- get_sentiment(text)
newdf$sentiment[i] <- sentiment
}
newdf
newdf <- data.frame(
text = data$text,
sentiment = rep(9.999, nrow(data)))
newdf
for (i in length(data$text)) {
# Sentiment analysis
sentiment <- get_sentiment(text)
newdf$sentiment[i] <- sentiment
}
newdf
length(data$text)
for (i in 1:length(data$text)) {
# Sentiment analysis
sentiment <- get_sentiment(text)
newdf$sentiment[i] <- sentiment
}
newdf
print(data$text[i])
for (i in 1:length(data$text)) {
print(data$text[i])
# Sentiment analysis
#sentiment <- get_sentiment(text)
#newdf$sentiment[i] <- sentiment
}
for (i in 1:length(data$text)) {
#print(data$text[i])
# Sentiment analysis
sentiment <- get_sentiment(data$text[i])
newdf$sentiment[i] <- sentiment
}
newdf
corpus = ""
corpus
corpus = data$text[1] + data$text[2]
corpus
mean(newdf$sentiment)
library(tidytext)
library(topicmodels)
library(tidyr)
library(ggplot2)
library(dplyr)
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis")
data("AssociatedPress")
ap_lda <- LDA(AssociatedPress,k =2,control=list(seed=1234)) #create two topic LDA model
ap_topics <- tidy( ap_lda,matrix ="beta") #Extract the per topic per word probabilities
ap_topics <- tidy( ap_lda,matrix ="beta") #Extract the per topic per word probabilities
ap_top_terms <- ap_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
ap_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes=(term,beta,fill=factor(topic))) +
ap_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.001 | topic2>0.001) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
ap_documents <- tidy(ap_lda, matrix='gamma')
ap_documents
tidy(AssociatedPress) %>% filter(document==6) %>%
arrange(desc(count))
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic2>0.003) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
ap_documents[ap_documents$topic ==2]
ap_documents[ap_documents$topic ==1]
ap_documents[ap_documents$topic ==1,]
ap_documents[ap_documents$topic ==2,]
ap_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot2(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
ap_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
ap_documents[ap_documents$topic ==1,]
ap_documents[ap_documents$topic ==2,]
ap_documents[ap_documents$document ==2,]
ap_documents[ap_documents$document ==2,]
ap_documents[ap_documents$document == 3,]
tidy(AssociatedPress) %>% filter(document==3) %>%
arrange(desc(count))
ap_documents[ap_documents$gamma >= 0.5,]
library(tm)
mytext <- DirSource("movie")
mytext <- DirSource("movies")
mytext <- DirSource("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Mining/dataset/movies")
mycorpus <- VCorpus(mytext)
library(tm)
mytext <- DirSource("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Mining/dataset/movies")
docs <- VCorpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,removePunctuation)
docs<-tm_map(docs,content_transformer(tolower))
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removeWords,stopwords("english")) #remove stop words
dtm <- DocumentTermMatrix(docs)
dtm <- DocumentTermMatrix(docs)
movie_lda <- LDA(dtm,k =2,control=list(seed=1234)) #create two topic LDA model
movie_topics <- tidy( ap_lda,matrix ="beta") #Extract the per topic per word probabilities
movie_top_terms <- movie_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
movie_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
dtm <- DocumentTermMatrix(docs)
movie_lda <- LDA(dtm,k =2,control=list(seed=1234)) #create two topic LDA model
movie_topics <- tidy( ap_lda,matrix ="beta") #Extract the per topic per word probabilities
movie_lda
movie_topcs
movie_topics
dtm <- DocumentTermMatrix(docs)
movie_lda <- LDA(dtm,k =2,control=list(seed=1234)) #create two topic LDA model
movie_topics <- tidy(movie_lda,matrix ="beta") #Extract the per topic per word probabilities
movie_top_terms <- movie_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
movie_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
movie_top_terms <- movie_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
movie_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
movie_top_terms <- movie_topics %>%
group_by (topic) %>%
top_n (3,beta) %>%
ungroup () %>%
arrange (topic, beta)
movie_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
dtm <- DocumentTermMatrix(docs)
movie_lda <- LDA(dtm,k=3,control=list(seed=1234)) #create two topic LDA model
movie_topics <- tidy(movie_lda,matrix ="beta") #Extract the per topic per word probabilities
movie_top_terms <- movie_topics %>%
group_by (topic) %>%
top_n (3,beta) %>%
ungroup () %>%
arrange (topic, beta)
movie_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
ap_lda <- LDA(AssociatedPress,k =4,control=list(seed=1234)) #create two topic LDA model
ap_topics <- tidy( ap_lda,matrix ="beta") #Extract the per topic per word probabilities
ap_top_terms <- ap_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
ap_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic3>0.003) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic4>0.003) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic2>0.003) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic3>0.003) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.003 | topic3>0.003) %>%
mutate(log_ratio = log2(topic3/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.004 | topic3>0.004) %>%
mutate(log_ratio = log2(topic3/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic2>0.004 | topic4>0.004) %>%
mutate(log_ratio = log2(topic4/topic2))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
ap_documents <- tidy(ap_lda, matrix='gamma')
ap_documents[ap_documents$gamma >= 0.5,]
tidy(AssociatedPress) %>% filter(document==15) %>%
arrange(desc(count))
dim(ap_documents)
ap_documents[ap_documents$document == 4,]
ap_documents %>% slice(document == 4)
ap_documents %>% slice(ap_documents$document == 4)
ap_documents[ap_documents$document == 4,]
ap_documents %>% subset(ap_documents$document == 4)
ap_documents %>% subset(document == 4)
ap_documents %>% subset(gamma == 0.5)
ap_documents %>% subset(gamma == 0.25)
ap_documents %>% subset(document == 4)
mytext2 <- DirSource("data")
doc2 <- VCorpus(mytext2)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs2<-tm_map(docs2,toSpace,"-")
mytext2 <- DirSource("data")
docs2 <- VCorpus(mytext2)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs2<-tm_map(docs2,toSpace,"-")
docs2<-tm_map(docs2,removePunctuation)
docs2<-tm_map(docs2,content_transformer(tolower))
docs2<-tm_map(docs2,removeNumbers)
docs2<-tm_map(docs2,removeWords,stopwords("english")) #remove stop words
dtm <- DocumentTermMatrix(docs)
movie_lda <- LDA(dtm,k=3,control=list(seed=1234)) #create two topic LDA model
movie_topics <- tidy(movie_lda,matrix ="beta") #Extract the per topic per word probabilities
dtm2 <- DocumentTermMatrix(docs2)
test_lda <- LDA(dtm2,k=2,control=list(seed=1234)) #create two topic LDA model
test_topics <- tidy(test_lda,matrix ="beta") #Extract the per topic per word probabilities
test_top_terms <- test_topics %>%
group_by (topic) %>%
top_n (3,beta) %>%
ungroup () %>%
arrange (topic, beta)
test_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
test_top_terms <- test_topics %>%
group_by (topic) %>%
top_n (10,beta) %>%
ungroup () %>%
arrange (topic, beta)
test_top_terms %>%
mutate(term=reorder(term,beta)) %>%
ggplot(aes(term,beta,fill=factor(topic))) +
geom_col(show.legend=F) +
facet_wrap(~ topic, scales="free") +
coord_flip()
beta_spread <- test_topics %>%
mutate(topic=paste0("topic",topic)) %>%
spread(topic,beta) %>%
filter(topic1>0.004 | topic2>0.004) %>%
mutate(log_ratio = log2(topic2/topic1))
beta_spread %>%
mutate(term=reorder(term,log_ratio)) %>%
ggplot(aes(term,log_ratio)) +
geom_col(show.legend=F) +
coord_flip()
mytext <-DirSource("data")
getwd()
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis")
mytext <-DirSource("data")
docs <- VCorpus(mytext)
docs <- tm_map(docs,content_transformer(tolower))
toSpace <-content_transformer(function(x, pattern) {
return ( gsub (pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs,content_transformer(tolower))
toSpace <-content_transformer(function(x, pattern) {
return ( gsub (pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <-tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, ". ")
docs <- tm_map(docs, toSpace, " -")
docs <- tm_map(docs, removePunctuation) #remove punctuation
docs <- tm_map(docs, removeNumbers) #Strip digits
docs <- tm_map(docs, removeWords , stopwords("english ")) #remove stopwords
docs <- tm_map(docs, removeWords , stopwords("english")) #remove stopwords
docs <- tm_map(docs, stripWhitespace ) #remove whitespace
tdm <-DocumentTermMatrix(docs) #Create document term matrix
tdm
#Present text data numerically, weighted TF-IDF
tdm.tfidf <- weightTfIdf(tdm)
tdm.tfidf <- removeSparseTerms(tdm.tfidf, 0.999)
tfidf.matrix <- as.matrix(tdm.tfidf)
dist.matrix <-dist(tfidf.matrix , method = "cosine")
library(proxy)
dist.matrix <-dist(tfidf.matrix , method = "cosine")
truth.K=2
library(dbscan)
install.packages('dbscan')
mytext <- DirSource("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis/data")
docs <- VCorpus(mytext)
docs <- tm_map(docs,content_transformer(tolower))
toSpace <-content_transformer(function(x, pattern) {
return ( gsub (pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <-tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, ". ")
docs <- tm_map(docs, toSpace, " -")
docs <- tm_map(docs, removePunctuation) #remove punctuation
docs <- tm_map(docs, removeNumbers) #Strip digits
docs <- tm_map(docs, removeWords , stopwords("english")) #remove stopwords
docs <- tm_map(docs, stripWhitespace ) #remove whitespace
tdm <-DocumentTermMatrix(docs) #Create document term matrix
tdm
tdm.tfidf <- weightTfIdf(tdm)
tdm.tfidf <- removeSparseTerms(tdm.tfidf, 0.999)
tfidf.matrix <- as.matrix(tdm.tfidf)
library(dbscan)
clustering.kmeans<- kmeans(tfidf.matrix , truth.K)
clusplot(as.matrix(dist.matrix),clustering.kmeans$cluster,color=T,shade=T,labels=2,lines=0)
mytext <- DirSource("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis/data")
inspect(docs)
mytext <- DirSource("data")
insepct(docs)
inspect(docs)
docs <- tm_map(docs,content_transformer(tolower))
inspect(docs)
docs <- tm_map(docs, toSpace, "-")
inspect(docs)
docs <-tm_map(docs, toSpace, ":")
inspect(docs)
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, ". ")
docs <- tm_map(docs, toSpace, " -")
inspect(docs)
docs <- tm_map(docs, removePunctuation) #remove punctuation
inspect(docs)
docs <- tm_map(docs, removeNumbers) #Strip digits
inspect(docs)
docs <- tm_map(docs, removeWords, stopwords("english")) #remove stopwords
inspect(docs)
docs <- tm_map(docs, stripWhitespace) #remove whitespace
inspect(docs)
tdm <-DocumentTermMatrix(docs) #Create document term matrix
tdm
tdm.tfidf <- weightTfIdf(tdm)
tdm.tfidf <- removeSparseTerms(tdm.tfidf, 0.999)
tfidf.matrix <- as.matrix(tdm.tfidf)
tfidf.matrix
mytext <- DirSource("data")
mytext
mytedocs <- VCorpus(mytext)
docs <- VCorpus(mytext)
docs
inspect(d0cs)
inspect(docs)
docs <- tm_map(docs,content_transformer(tolower))
toSpace <-content_transformer(function(x, pattern) {
return ( gsub (pattern, " ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <-tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, ". ")
docs <- tm_map(docs, toSpace, " -")
docs <- tm_map(docs, removePunctuation) #remove punctuation
docs <- tm_map(docs, removeNumbers) #Strip digits
docs <- tm_map(docs, removeWords, stopwords("english")) #remove stopwords
docs <- tm_map(docs, stripWhitespace) #remove whitespace
inspect(docs)
tdm <-DocumentTermMatrix(docs) #Create document term matrix
tdm
tdm.tfidf <- weightTfIdf(tdm)
tdm.tfidf <- removeSparseTerms(tdm.tfidf, 0.999)
tfidf.matrix <- as.matrix(tdm.tfidf)
tfidf.matrix
getwd()
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis")
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Analysis")
getwd()
mytext <- DirSource("data")
docs <- VCorpus(mytext)
docs <- tm_map(docs,content_transformer(tolower))
toSpace <-content_transformer(function(x, pattern) {
return ( gsub (pattern, " ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <-tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, ". ")
docs <- tm_map(docs, toSpace, " -")
docs <- tm_map(docs, removePunctuation) #remove punctuation
docs <- tm_map(docs, removeNumbers) #Strip digits
docs <- tm_map(docs, removeWords, stopwords("english")) #remove stopwords
docs <- tm_map(docs, stripWhitespace) #remove whitespace
tdm <-DocumentTermMatrix(docs) #Create document term matrix
tdm
tdm.tfidf <- weightTfIdf(tdm)
tdm.tfidf <- removeSparseTerms(tdm.tfidf, 0.999)
tfidf.matrix <- as.matrix(tdm.tfidf)
tfidf.matrix
mytext <- DirSource("TextFile")
m
mytext
