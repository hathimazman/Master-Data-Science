---
title: "Text Exploration and Text Mining"
output:
  pdf_document: default
knitr:
  opts_chunk: 
    echo: true
    include: true
    message: false  # Optional: Hide messages
    warning: false  # Optional: Hide warnings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Mining")
```

# Text Mining

## Types of corpus

-   **VCorpus**: Volatile Corpus, stored in memory.
-   **PCorpus**: Persistent Corpus, stored on disk.

## Predefined Sources of in package

-   **VectorSource**: Vector of text.
-   **DataframeSource**: Data frame of text.
-   **DirSource**: Directory of text files

## Part 1: Extract Text from Files

```{r}
eg1 <- read.table("dataset/GC.txt", fill=T,header=F) #Data CG.txt
eg1[1,]

eg2 <- read.csv("dataset/GC.csv",header=F) #Data CG.csv
eg2
```

### Using tm Package

```{r}
library(tm)
eg3 <- c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext <-VectorSource(eg3)
mycorpus <- VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
```

### Example using VectorSource

```{r}
eg4<-t(eg1) #From example 1 
a<-sapply(1:7,function(x) trimws(paste(eg4[,x],collapse=" "),"right")) 
mytext<-VectorSource(a) 
mycorpus<-VCorpus(mytext) 
#inspect(mycorpus) 
as.character(mycorpus[[1]])
```

### Example using DataFrameSource

```{r}
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv 
docs<-data.frame(doc_id=c("doc_1","doc_2"),                 
                 text=c(as.character(eg5[1,]),as.character(eg5[2,])),                 
                 dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F) 
mytext<-DataframeSource(docs) 
mycorpus<-VCorpus(mytext) 
inspect(mycorpus)
```

### Example using DirSource

```{r}
mytext<-DirSource("dataset/movies") 
mycorpus<-VCorpus(mytext) 
#inspect(mycorpus) 
as.character(mycorpus[[1]])
```

## Part 2: Web scrapping

```{r}
eg6<-readLines("https://en.wikipedia.org/wiki/Data_science") 
#eg6[grep("\\h2",eg6)] 
#eg6[grep("\\p",eg6)] #paragraph
```

### Using library XML

```{r}
library(XML) 
doc<-htmlParse(eg6) 
doc.text<-unlist(xpathApply(doc,'//p',xmlValue)) 
unlist(xpathApply(doc,'//h2',xmlValue))
```

### Using library httr

```{r}
library(httr)
eg7<-GET("https://www.edureka.co/blog/what-is-data-science/") 
doc<-htmlParse(eg7) 
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
```

### Using library rvest

```{r}
library(rvest) 
eg8<-read_html("https://www.edureka.co/blog/what-is-data-science/") 
nodes<-html_nodes(eg8,'.col-lg-9 :nth-child(1)') 
texts<-html_text(nodes)
```

### Selecting multiple pages

`pages <- paste0("--url--&page=",0:9)`

```{r}
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
#pages<-paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix#=skincare%2Caps%2C268&r ef=nb_sb_noss_1&page=',0:9) 
eg10<-read_html(pages[1]) 
nodes<-html_nodes(eg10,'.a-price-whole') 
texts<-html_text(nodes)
texts
```

```{r}
Price <- function(page) {
  url <- read_html(page)
  nodes <- html_nodes(url, '.a-price-whole')
  html_text(nodes)
}

sapply(pages,Price)
do.call("c", lapply(pages,Price))
```

```{r}
pricelist <- do.call("c", lapply(pages,Price))
pricelist <- as.numeric(pricelist)
mean(pricelist)
```

# Text Exploration

```{r}
ww<-c("statistics","estate","castrate","catalyst","Statistics")
ss<-c("I like statistics","I like bananas","Estates and statues are expensive")
```

### 1. grep() -give the location of pattern

```{r}
grep(pattern="stat",x=ww) #x is the document, will return the location only
grep(pattern="stat",x=ww,ignore.case=T) #ignore the capital/small letter, will return the location only
grep(pattern="stat",x=ww,ignore.case=T,value=T) #ignore the capital/small letter, return to that particular words
```

### 2. grepl() - give logical expression

Return True/False

```{r}
grepl(pattern="stat",x=ww) #Return true/false
grepl(pattern="stat",x=ss)
```

### 3. regexpr() - return a vector of two attributes; position of the first match and its length

```{r}
regexpr(pattern="stat",ww, ignore.case = T)
print("===separator===")
regexpr(pattern="stat",ss)
```

### 4. gregexpr()

g infront of `gregexpr()` means 'global'

```{r}
gregexpr(pattern="stat",ss)
```

### 5. regexec()

Find full and partial search

-   stat
-   st
-   at

```{r}
regexec(pattern="(st)(at)",ww)
```

### 6. sub()

```{r}
sub("stat","STAT",ww,ignore.case=T)
sub("stat","STAT",ss,ignore.case=T)
```

### 7. gsub()

```{r}
gsub("stat","STAT",ss,ignore.case=T)
```

## Stringr

```{r}
library(stringr)
```

### Common function in stringr

-   `str_length()` - gives the length of that string
-   `str_split()` - split the function by space & return the list
-   `str_c()` - combine string to become a long list

```{r}
str_length("This is STQD6114") 
str_split("This is STQD6114"," ") 
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one", collapse=",") #combine the string to be one sentences
```

```{r}
x<-c("Apple","Banana","Pear")
```

`str_sub()` gives subset

```{r}
str_sub(x,1,3) #Gives from 1st to 3rd letter
str_sub(x,-3,-1) #Gives the last three letter
```

```{r}
str_to_upper(x) #Return the string to upper case letter
str_to_lower(x) #Return the string to lower case letter
str_to_title("unstructured data analytics") #Return upper case letter to the string 
str_to_title(x)
```

`str_view()` give the output in another browser

```{r}
str_view(fruit,"an") #view the pattern  of dataset 
str_view_all(fruit,"an") #view all data
```

"." refers to anything

```{r}
str_view(fruit,".a.") #refers to dataset fruit, find any fruit that have letter a
str_view(x,".a.")
str_view(sentences,".a.") #refers to dataset sentences, find any sentence that have letter a that is seen 1st time 
str_view_all(sentences,"\'") #find the symbol('), put backlash(\) or not is ok
```

### Anchor

`^` refers to the start of a string, `$` refers to the end of a string

```{r}
str_view(x,"^A")
str_view(x,"a$")
str_view(fruit,"^a") #find the fruit that has first word "a" in fruit dataset
str_view(fruit,"a$") #find the fruit that has end word "a" in fruit dataset
str_view(fruit,"^...$") #find the fruits with 3 character(letter), doesn't matter what letter as a start and end

```

### space, tab, newlines

`\\b` - boundary,

`\\d` - digits

`\\s` - white space

```{r}
ee<-c("sum","summarize","rowsum","summary")
str_view(ee,"sum")
str_view(ee,"\\bsum") #if let say we want to put boundaries, means the earlier/start words with sum. So, rowsum is not included
str_view(ee, "^(sum)")
str_view(ee,"sum\\b") #if let say we want to put boundaries, means the end words with sum.
```

```{r}
str_view(sentences,"\\d") #find digits in dataset sentences
ss<-c("This is a class with students","There are 18 students","This class is from 11.00 am")
str_view(ss,"\\d") #Find any sentences that have digits
str_view(ss,"\\s") #Find any sentences that have white space
```

```{r}
str_view(fruit,"[abc]") #[abc] match to all a/b/c. Can also use "(a|b|c)"
str_view(fruit,"^[abc]") #any fruit that is started with any a/b/c
str_view(fruit,"^(g|h)")
```

### Repetition

`?` means 0 or 1 repetition

`+` means 1 or more repetition

`*` means 0 or more repetition

`{n}` means n times repetition

`{n,}` means n or more times repetition

`{,m}` means m or less times repetition

`{n,m}` means between n to m times repetition

```{r}
ex<-"aabbbccddddeeeee"
str_view(ex,"aab?") #gives 0 or 1 aab
str_view(ex,"aac?") #gives 0 or 1 aac. The output gives aa because can be 0
str_view(ex,"(a|d){2}") #Find a or d that occur 2 times
str_view(ex,"de+") #Find d and e, the letter e can be once or more
str_view(ex,"de+?") #Find d and e, and gives the shortest
str_view(ss,"\\d+") #Find digits at least once
str_view(ss,"\\d{2,}") #Find digits, 2 times or more
```

### Grouping and backreferencing

`\\1` refers to first pattern in `()`

```{r}
str_view(fruit,"(a).\\1") #Find a, after a any letter (one dot=one letter),then repeat a once
str_view(fruit,"(a).+\\1") #Follow above, between a must have more than one repetition (the longest repetition)
str_view(fruit,"(a)(.)\\1\\2") #Find a, followed by any characters,then repeat a gain, then repeat any characters
str_view(fruit,"(.)(.)\\2\\1") #Find any two character, repeat the second one first, then repeat the first one

```

exercise

```{r}
str_view(words,"^(.).*\\1$") #Find any character, that is started with anything, and have any character inside (can be 0/more because 
#use *),and end with the 1st letter
str_view(words,"(..).*\\1") #Find a pair of characters that is repeat in that word (will end with that pair of words)

```

Other function in stringr

```{r}
str_detect(fruit,"e") #Return true/false that consists of words that have e inside
str_detect(fruit,"[aeiou]$") #a/e/i/o/u at the end
str_count(fruit,"e") #Cound the letter e in the word
fruit[5] #just check the fruit no 5
str_replace(fruit,"^a","A") #replace a with capital letter A
str_replace_all(fruit, c("^a"="A", "^e"="E")) #replace a with capital letter A, e with E
```

## Preprocessing Data

### Steps for pre-processing data

1.  Create corpus
2.  Cleaning raw data
    -   transformation of words - lowering case
    -   removal of special characters
    -   removal of stopwords
    -   removal of extra spaces / punctuation / numbers
3.  Tokenization
4.  Creating Term-Document Matrix

### Create corpus

```{r}
library(tm)
docs <- Corpus(VectorSource(sentences))
writeLines(as.character(docs[[30]]))
```

### Cleaning

```{r}
getTransformations()
```

#### Create custom transformation

```{r}
toSpace <- content_transformer(function(x,pattern){
  return(gsub(pattern," ", x))
})
```

```{r}
as.character(docs[[133]]) #check line 133 
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
```

```{r}
docs<-tm_map(docs,removePunctuation)
docs<-tm_map(docs,content_transformer(tolower))
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removeWords,stopwords("english")) #remove stop words
```

```{r}
as.character(docs[[2]])
```

```{r}
docs<-tm_map(docs,removeWords,"gp")
docs<-tm_map(docs,stripWhitespace)
```

### Tokenization

stemming vs lemmatization

```{r}
library(SnowballC)
library(textstem)
docs2<-tm_map(docs,stemDocument) #for stemming the documents
docs21 <- tm_map(docs,lemmatize_strings)
docs22 <- tm_map(docs2, lemmatize_strings)
```

```{r}
library(textstem)
docs3<-stem_strings(docs)
a<-unlist(str_split(docs3,"[,]"))
docs4<-lemmatize_strings(docs)
b<-unlist(str_split(docs4,"[,]"))
```

### Create term-document matrix

```{r}
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
```

```{r}
dtm<-DocumentTermMatrix(docs,control=list(wordLengths=c(2,20),
                                          bounds=list(global=c(2,30))))
```

```{r}
inspect(dtm[1:2,1:100])
freq<-colSums(as.matrix(dtm))
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
```

```{r}
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
head(wf)

# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),]
head(wf)
```

Find frequency terms, and associations

```{r}
findFreqTerms(dtm,lowfreq=10)
findAssocs(dtm,"get",0.3)
```

## Visualization

```{r}
library(ggplot2)
Subs<-subset(wf,FREQ>=10)
ggplot(Subs,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(wf,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
```

```{r}
library(wordcloud2)
wordcloud2(wf)
wordcloud2(wf,size=0.5)
wordcloud2(wf,size=0.5,color="random-light",backgroundColor="black")
wordcloud2(wf,shape="star",size=0.5)
wordcloud2(wf,figPath="love.png",color="random-light")
#wordcloud2(wf,figPath="cat.png",color="skyblue",backgroundColor="black")
letterCloud(wf,word="R",color="random-light",backgroundColor="black")
letterCloud(wf,word="SDA",color="random-light",backgroundColor="black")
```
