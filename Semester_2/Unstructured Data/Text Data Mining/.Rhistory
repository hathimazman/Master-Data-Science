#inspect(mycorpus)
as.character(mycorpus[[1]])
eg6<-readLines("https://en.wikipedia.org/wiki/Data_science")
#eg6[grep("\\h2",eg6)]
#eg6[grep("\\p",eg6)] #paragraph
library(XML)
doc<-htmlParse(eg6)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
unlist(xpathApply(doc,'//h2',xmlValue))
library(httr)
eg7<-GET("https://www.edureka.co/blog/what-is-data-science/")
doc<-htmlParse(eg7)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
eg1 <- read.table("dataset/GC.txt", fill=T,header=F) #Data CG.txt
eg1[1,]
eg2 <- read.csv("dataset/GC.csv",header=F) #Data CG.csv
eg2[1,]
eg1 <- read.table("dataset/GC.txt", fill=T,header=F) #Data CG.txt
eg1[1,]
eg2 <- read.csv("dataset/GC.csv",header=F) #Data CG.csv
eg2[1,]
eg1[1,]
library(tm)
eg3 <- c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext <-VectorSource(eg3)
mycorpus <- VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg4<-t(eg1) #From example 1
a<-sapply(1:7,function(x) trimws(paste(eg4[,x],collapse=" "),"right"))
mytext<-VectorSource(a)
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv
docs<-data.frame(doc_id=c("doc_1","doc_2"),
text=c(as.character(eg5[1,]),as.character(eg5[2,])),
dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext<-DataframeSource(docs)
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
mytext<-DirSource("dataset/movies")
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg6<-readLines("https://en.wikipedia.org/wiki/Data_science")
#eg6[grep("\\h2",eg6)]
#eg6[grep("\\p",eg6)] #paragraph
library(XML)
doc<-htmlParse(eg6)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
unlist(xpathApply(doc,'//h2',xmlValue))
library(httr)
eg7<-GET("https://www.edureka.co/blog/what-is-data-science/")
doc<-htmlParse(eg7)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
library(rvest)
eg8<-read_html("https://www.edureka.co/blog/what-is-data-science/")
nodes<-html_nodes(eg8,'.col-lg-9 :nth-child(1)')
texts<-html_text(nodes)
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb",0:9)
pages
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
pages
eg10<-read_html(pages[1])
nodes<-html_nodes(eg10,'.a-price-whole')
texts<-html_text(nodes)
texts
knitr::opts_chunk$set(echo = TRUE)
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Text Data Mining")
eg1 <- read.table("dataset/GC.txt", fill=T,header=F) #Data CG.txt
eg1[1,]
eg2 <- read.csv("dataset/GC.csv",header=F) #Data CG.csv
eg2[1,]
library(tm)
eg3 <- c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext <-VectorSource(eg3)
mycorpus <- VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg4<-t(eg1) #From example 1
a<-sapply(1:7,function(x) trimws(paste(eg4[,x],collapse=" "),"right"))
mytext<-VectorSource(a)
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv
docs<-data.frame(doc_id=c("doc_1","doc_2"),
text=c(as.character(eg5[1,]),as.character(eg5[2,])),
dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext<-DataframeSource(docs)
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
mytext<-DirSource("dataset/movies")
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
as.character(mycorpus[[1]])
eg6<-readLines("https://en.wikipedia.org/wiki/Data_science")
#eg6[grep("\\h2",eg6)]
#eg6[grep("\\p",eg6)] #paragraph
library(XML)
doc<-htmlParse(eg6)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
unlist(xpathApply(doc,'//h2',xmlValue))
library(httr)
eg7<-GET("https://www.edureka.co/blog/what-is-data-science/")
doc<-htmlParse(eg7)
doc.text<-unlist(xpathApply(doc,'//p',xmlValue))
library(rvest)
eg8<-read_html("https://www.edureka.co/blog/what-is-data-science/")
nodes<-html_nodes(eg8,'.col-lg-9 :nth-child(1)')
texts<-html_text(nodes)
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
pages<-paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix=skincare%2Caps%2C268&r ef=nb_sb_noss_1&page=',0:9)
eg10<-read_html(pages[1])
eg10<-read_html(pages[1])
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
#pages<-paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix#=skincare%2Caps%2C268&r ef=nb_sb_noss_1&page=',0:9)
eg10<-read_html(pages[1])
#pages<-paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix#=skincare%2Caps%2C268&r ef=nb_sb_noss_1&page=',0:9)
eg10<-read_html(pages[1])
eg10
nodes<-html_nodes(eg10,'.a-price-whole')
nodes
texts
texts<-html_text(nodes)
texts
pages <- paste0("https://www.amazon.com/s?k=skincare&ref=nav_bb_sb&page=",0:9)
#pages<-paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix#=skincare%2Caps%2C268&r ef=nb_sb_noss_1&page=',0:9)
eg10<-read_html(pages[1])
nodes<-html_nodes(eg10,'.a-price-whole')
texts<-html_text(nodes)
texts
Price <- function(page) {
url <- read_html(page)
nodes <- html_nodes(url, '.a-price-whole')
html_text(nodes)
}
sapply(pages,Price)
do.call("c", lapply(pages,Price))
pricelist <- do.call("c", lapply(pages,Price))
str(pricelist)
pricelist <- as.numeric(pricelist)
pricelist
mean(pricelist)
ww<-c("statistics","estate","castrate","catalyst","Statistics")
ss<-c("I like statistics","I like bananas","Estates and statues are expensive")
grep(pattern="stat",x=ww) #x is the document, will return the location only
grep(pattern="stat",x=ww,ignore.case=T) #ignore the capital/small letter, will return the location only
grep(pattern="stat",x=ww,ignore.case=T,value=T) #ignore the capital/small letter, return to that particular words
grepl(pattern="stat",x=ww) #Return true/false
grepl(pattern="stat",x=ss)
regexpr(pattern="stat",ww)
regexpr(pattern="stat",ww)
regexpr(pattern="stat",ss)
regexpr(pattern="stat",ww)
grep(pattern="stat",x=ww) #x is the document, will return the location only
grep(pattern="stat",x=ww,ignore.case=T) #ignore the capital/small letter, will return the location only
grep(pattern="stat",x=ww,ignore.case=T,value=T) #ignore the capital/small letter, return to that particular words
regexpr(pattern="stat",ww)
regexpr(pattern="stat",ss)
regexpr(pattern="stat",ww)
print("===separator===")
regexpr(pattern="stat",ss)
gregexpr(pattern="stat",ss)
gregexpr(pattern="stat",ss)
regexpr(pattern="stat",ww, ignore.case = T)
regexpr(pattern="stat",ww, ignore.case = T)
print("===separator===")
regexpr(pattern="stat",ss)
regexec(pattern="(st)(at)",ww)
sub("stat","STAT",ww,ignore.case=T)
gsub("stat","STAT",ss,ignore.case=T)
sub("stat","STAT",ss,ignore.case=T)
library(stringr)
words
library(stringr)
str_length("This is STQD6114") #str_length()-gives the length of that string
str_split(sentences," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
# str_split(sentences," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
str_length("This is STQD6114") #str_length()-gives the length of that string
# str_split(sentences," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_length("This is STQD6114") #str_length()-gives the length of that string
str_split("This is STQD6114"," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_c("one for all","All for one",collapse="") #combine the string to be one sentences
str_c("one for all","All for one",collapse=" ") #combine the string to be one sentences
str_c("one for all","All for one",collapse=", ") #combine the string to be one sentences
str_c("one for all","All for one",collapse=", ") #combine the string to be one sentences
str_length("This is STQD6114") #str_length()-gives the length of that string
str_split("This is STQD6114"," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_length("This is STQD6114")
str_split("This is STQD6114"," ") #str_split()-split the function by space & return the list
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
x<-c("Apple","Banana","Pear")
str_sub(x,1,3) #Gives from 1st to 3rd letter
str_sub(x,-3,-1) #Gives the last three letter
str_to_upper(x) #Return the string to upper case letter
str_to_lower(x) #Return the string to lower case letter
str_to_title("unstructured data analytics") #Return upper case letter to the string
str_to_title(x)
str_length("This is STQD6114")
str_split("This is STQD6114"," ")
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_length("This is STQD6114")
str_split("This is STQD6114"," ")
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all , All for one",collapse=",") #combine the string to be one sentences
str_length("This is STQD6114")
str_split("This is STQD6114"," ")
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_length("This is STQD6114")
str_split("This is STQD6114"," ")
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one",collapse="") #combine the string to be one sentences
str_length("This is STQD6114")
str_split("This is STQD6114"," ")
str_c("a","b","c") #combine string to become a long ist
str_c("A",c("li","bu","ngry")) #combine A to each vector
str_c("one for all","All for one",sep=",") #combine these string and separate by comma
str_c("one for all","All for one") #combine the string to be one sentences
str_c("one for all","All for one", collapse=",") #combine the string to be one sentences
str_view(fruit,"an") #view the pattern (for the first time) of dataset
str_view_all(fruit,"an") #view all pattern (including repeated observation)
str_view(fruit,"an") #view the pattern (for the first time) of dataset
str_view_all(fruit,"an") #view all pattern (including repeated observation)
str_view(fruit,"an") #view the pattern (for the first time) of dataset
str_view(fruit,"an") #view the pattern  of dataset
str_view_all(fruit,"an") #view all data
str_view(fruit,".a.") #refers to dataset fruit, find any fruit that have letter a
str_view(x,".a.")
str_view(sentences,".a.") #refers to dataset sentences, find any sentence that have letter a that is seen 1st time
str_view_all(sentences,"\'") #find the symbol('), put backlash(\) or not is ok
str_view(x,"^A")
str_view(x,"a$")
str_view(fruit,"^a") #find the fruit that has first word "a" in fruit dataset
str_view(fruit,"a$") #find the fruit that has end word "a" in fruit dataset
str_view_all(fruit,"^...$") #find the fruits with 3 character(letter), doesn't matter what letter as a start and end
str_view(x,"^A")
str_view(x,"a$")
str_view(fruit,"^a") #find the fruit that has first word "a" in fruit dataset
str_view(fruit,"a$") #find the fruit that has end word "a" in fruit dataset
str_view_all(fruit,"^...$") #find the fruits with 3 character(letter), doesn't matter what letter as a start and end
str_view_all(fruit,"^...$") #find the fruits with 3 character(letter), doesn't matter what letter as a start and end
str_view(fruit,"^...$") #find the fruits with 3 character(letter), doesn't matter what letter as a start and end
ee<-c("sum","summarize","rowsum","summary")
str_view(ee,"sum")
str_view(ee,"\\bsum") #if let say we want to put boundaries, means the earlier/start words with sum. So, rowsum is not included
str_view(fruit,"^a") #find the fruit that has first word "a" in fruit dataset
str_view(ee, "^(sum)")
str_view(ee, "^(sum)")
str_view(sentences,"\\d") #find digits in dataset sentences
str_view(sentences,"\\d") #find digits in dataset sentences
ss<-c("This is a class with students","There are 18 students","This class is from 11.00 am")
str_view(ss,"\\d") #Find any sentences that have digits
str_view(ss,"\\s") #Find any sentences that have white space
str_view_all(fruit,"[abc]") #[abc] match to all a/b/c. Can also use "(a|b|c)"
str_view(fruit,"[abc]") #[abc] match to all a/b/c. Can also use "(a|b|c)"
str_view(fruit,"^[abc]") #any fruit that is started with any a/b/c
str_view(fruit,"^(g|h)")
ex<-"aabbbccddddeeeee"
str_view(ex,"aab?") #gives 0 or 1 aab
str_view(ex,"aac?") #gives 0 or 1 aac. The output gives aa because can be 0
str_view(ex,"(a|d){2}") #Find a or d that occur 2 times
str_view_all(ex,"de+") #Find d and e, the letter e can be once or more
str_view_all(ex,"de+?") #Find d and e, and gives the shortest
str_view_all(ss,"\\d+") #Find digits at least once
str_view(ex,"(a|d){2}") #Find a or d that occur 2 times
str_view(ex,"de+") #Find d and e, the letter e can be once or more
ex<-"aabbbccddddeeeee"
str_view(ex,"aab?") #gives 0 or 1 aab
str_view(ex,"aac?") #gives 0 or 1 aac. The output gives aa because can be 0
str_view(ex,"(a|d){2}") #Find a or d that occur 2 times
str_view(ex,"de+") #Find d and e, the letter e can be once or more
str_view(ex,"de+?") #Find d and e, and gives the shortest
str_view(ss,"\\d+") #Find digits at least once
str_view(ss,"\\d{2,}") #Find digits, 2 times or more
str_view(fruit,"(a).\\1") #Find a, after a any letter (one dot=one letter),then repeat a once
str_view(fruit,"(a).+\\1") #Follow above, between a must have more than one repetition (the longest repetition)
str_view(fruit,"(a)(.)\\1\\2") #Find a, followed by any characters,then repeat a gain, then repeat any characters
str_view(words,"^(.).*\\1$") #Find any character, that is started with anything, and have any character inside (can be 0/more because
#use *),and end with the 1st letter
str_view(words,"(..).*\\1") #Find a pair of characters that is repeat in that word (will end with that pair of words)
str_detect(fruit,"e") #Return true/false that consists of words that have e inside
str_detect(fruit,"[aeiou]$") #a/e/i/o/u at the end
docs <- Corpus(VectorSource(sentences))
docs
writeLines(as.character(docs[[1]]))
writeLines(as.character(docs[[30]]))
writeLines(docs[[30]])
getTransformations()
getTransformations()
library(tm)
docs <- Corpus(VectorSource(sentences))
writeLines(as.character(docs[[30]]))
getTransformations()
return(gsub(pattern," ",x))
toSpace <- content_transformer(function(x,pattern)) {
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
as.character(docs[[133]]) #check line 133
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
as.character(docs[[133]]) #check line 133
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
as.character(docs[[133]]) #check line 133
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
as.character(docs[[134]])
as.character(docs[[135]])
as.character(docs[[133]]) #check line 133
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
docs<-tm_map(docs,removePunctuation)
docs<-tm_map(docs,content_transformer(tolower))
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removeWords,stopwords("english")) #remove stop words
as.character(docs[[133]])
as.character(docs[[2]])
docs<-tm_map(docs,removeWords,"gp")
docs<-tm_map(docs,stripWhitespace)
as.character(docs[[2]])
library(SnowballC)
docs2<-tm_map(docs,stemDocument) #for stemming the documents
docs2<-tm_map(docs,stemDocument) #for stemming the documents
docs
inspect(docs)
inspect(docs2)
library(textstem)
library(textstem)
docs3<-stem_strings(docs)
a<-unlist(str_split(docs3,"[,]"))
a
docs4<-lemmatize_strings(docs)
b
docs4<-lemmatize_strings(docs)
b
docs4<-lemmatize_strings(docs)
docs21 <- tm_map(docs,lemmatize_strings)
docs2<-tm_map(docs,stemDocument) #for stemming the documents
inspect(docs2)
as.character(docs2[[133]])
as.character(docs21[[133]])
as.character(docs2[[120]])
as.character(docs21[[120]])
?lemmatize_strings
docs22 <- tm_map(docs2, lemmatize_strings)
as.character(docs22[[120]])
as.character(docs[[4]])
as.character(docs2[[4]])
as.character(docs2[[10]])
as.character(docs[[10]])
as.character(docs2[[10]])
as.character(docs21[[10]])
as.character(docs22[[10]])
dtm<-DocumentTermMatrix(docs)
inspect(dtm[1:2,1:100])
dtm
inspect(dtm[1:2,1:100])
freq<-colSums(as.matrix(dtm))
f
freq
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
ord
head(ord)
freq[head(ord)]
dtm
inspect(dtm)
freq
ord
str(freq)
freq[206]
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
fre[207]
fre1[207]
freq[207]
freq[head(ord)]
dtm<-DocumentTermMatrix(docs,control=list(wordLengths=c(2,20),
bounds=list(global=c(2,30))))
dtm
inspect(dtm[1:2,1:100])
freq<-colSums(as.matrix(dtm))
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
wf<-data.frame(names(freq),freq)
wf
names(wf)<-c("TERM","FREQ")
head(wf)
order(wf, decreasing=T)
findFreqTerms(dtm,lowfreq=10)
findAssocs(dtm,"get",0.3)
findFreqTerms(dtm,lowfreq=10)
findAssocs(dtm,"get",0.3)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
head(wf)
sort(wf,decreasing=T)
sort(wf,decreasing=T)
sort(wf$FREQ,decreasing=T)
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),]
wf
head(wf)
library(ggplot2)
Subs<-subset(wf,FREQ>=10)
ggplot(Subs,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(wf,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
library(ggplot2)
Subs<-subset(wf,FREQ>=10)
ggplot(Subs,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(wf,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
library(wordcloud)
wordcloud(names(freq),freq) #in general
wordcloud(names(freq),freq.min.freq=10) #if we want to focus on the min freq of 10
wordcloud(names(freq),freq,colors=brewer.pal(8,"Darker"))
library(wordcloud)
wordcloud(names(freq),freq) #in general
wordcloud(names(freq),freq.min.freq=10) #if we want to focus on the min freq of 10
wordcloud(names(freq),freq,colors=brewer.pal(8,"Darker"))
library(wordcloud)
wordcloud(names(freq),freq) #in general
#wordcloud(names(freq),freq.min.freq=10) #if we want to focus on the min freq of 10
#wordcloud(names(freq),freq,colors=brewer.pal(8,"Darker"))
#wordcloud(names(freq),freq,colors=brewer.pal(12,"Paired"))
library(wordcloud2)
wordcloud2(wf)
wordcloud2(wf,size=0.5)
wordcloud2(wf,size=0.5,color="random-light",backgroundColor="black")
wordcloud2(wf,shape="star",size=0.5)
wordcloud2(wf,figPath="love.png",color="skyblue",backgroundColor="black")
library(wordcloud2)
wordcloud2(wf)
wordcloud2(wf,size=0.5)
wordcloud2(wf,size=0.5,color="random-light",backgroundColor="black")
wordcloud2(wf,shape="star",size=0.5)
wordcloud2(wf,figPath="love.png",color="skyblue",backgroundColor="black")
wordcloud2(wf,figPath="love.png",color="skyblue",backgroundColor="black")
wordcloud2(wf,figPath="love.jpg",color="skyblue",backgroundColor="black")
wordcloud2(wf,figPath="love.jpg",color="skyblue",backgroundColor="black")
getwd()
wordcloud2(wf,figPath="love.png",color="skyblue",backgroundColor="black")
wordcloud2(wf,figPath="love.png",color="skyblue")
wordcloud2(wf,figPath="love.png",color="random-light")
wordcloud2(wf,figPath="love.png",color="random-light")
library(wordcloud2)
wordcloud2(wf)
wordcloud2(wf,size=0.5)
wordcloud2(wf,size=0.5,color="random-light",backgroundColor="black")
wordcloud2(wf,shape="star",size=0.5)
wordcloud2(wf,figPath="love.png",color="random-light")
#wordcloud2(wf,figPath="cat.png",color="skyblue",backgroundColor="black")
#wordcloud2(wf,figPath="cat.png",color="skyblue",backgroundColor="black")
letterCloud(wf,word="R",color="random-light",backgroundColor="black")
letterCloud(wf,word="SDA",color="random-light",backgroundColor="black")
#wordcloud2(wf,figPath="cat.png",color="skyblue",backgroundColor="black")
letterCloud(wf,word="R",color="random-light",backgroundColor="black")
library(wordcloud2)
wordcloud2(wf)
wordcloud2(wf,size=0.5)
wordcloud2(wf,size=0.5,color="random-light",backgroundColor="black")
wordcloud2(wf,shape="star",size=0.5)
wordcloud2(wf,figPath="love.png",color="random-light")
#wordcloud2(wf,figPath="cat.png",color="skyblue",backgroundColor="black")
letterCloud(wf,word="R",color="random-light",backgroundColor="black")
letterCloud(wf,word="SDA",color="random-light",backgroundColor="black")
