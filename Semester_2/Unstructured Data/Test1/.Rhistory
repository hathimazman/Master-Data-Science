bbc<-t(bbc)
bbc<-sapply(1:ncol(bbc), function(x)
trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-VCorpus(mytext)
#bbc = readLines("BBCnews.txt", encoding = "UTF-8")
#docs = Corpus(VectorSource(bbc))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
bbc <- read.table("BBCnews.txt", fill=T,header = F)
bbc<-t(bbc)
bbc<-sapply(1:ncol(bbc), function(x)
trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
#bbc = readLines("BBCnews.txt", encoding = "UTF-8")
#docs = Corpus(VectorSource(bbc))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
bbc <- read.table("BBCnews.txt", fill=T,header = F)
bbc<-t(bbc)
bbc<-sapply(1:ncol(bbc), function(x)
trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
#bbc = readLines("BBCnews.txt", encoding = "UTF-8")
#docs = Corpus(VectorSource(bbc))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
bbc <- read.table("BBCnews.txt", fill=T,header = F)
bbc<-t(bbc)
bbc<-sapply(1:ncol(bbc), function(x)
trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- read.table("BBCnews.txt", fill=T,header = F)
bbc<-t(bbc)
bbc<-sapply(1:ncol(bbc), function(x)
trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
dtm
insepct(dtm)
inspect(dtm)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
wordcloud2(wf)
ggplot(wf,aes(x=TERM,y=FREQ)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45,hjust=1))
findAssocs(dtm, "coast", 0.3)
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv
docs<-data.frame(doc_id=c("doc_1","doc_2"),
text=c(as.character(eg5[1,]),as.character(eg5[2,])),
dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext<-DataframeSource(docs)
mycorpus<-VCorpus(mytext)
#inspect(mycorpus)
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv
docs<-data.frame(doc_id=c("doc_1","doc_2"),
text=c(as.character(eg5[1,]),as.character(eg5[2,])),
dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext<-DataframeSource(docs)
mycorpus<-VCorpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[1]])
eg5<-read.csv("dataset/doc6.csv",header=F) #Using doc6.csv
docs<-data.frame(doc_id=c("doc_1","doc_2"),
text=c(as.character(eg5[1,]),as.character(eg5[2,])),
dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext<-DataframeSource(docs)
mycorpus<-VCorpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[2]])
bbc <- readLines("BBCnews.txt", encoding = "UTF-8")
#bbc <- read.table("BBCnews.txt", fill=T,header = F)
#bbc<-t(bbc)
#bbc<-sapply(1:ncol(bbc), function(x)
#  trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- readLines("BBCnews.txt")
#bbc <- read.table("BBCnews.txt", fill=T,header = F)
#bbc<-t(bbc)
#bbc<-sapply(1:ncol(bbc), function(x)
#  trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
bbc <- readLines("BBCnews.txt")
#bbc <- read.table("BBCnews.txt", fill=T,header = F)
#bbc<-t(bbc)
#bbc<-sapply(1:ncol(bbc), function(x)
#  trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- readLines("BBCnews.txt")
#bbc <- read.table("BBCnews.txt", fill=T,header = F)
#bbc<-t(bbc)
#bbc<-sapply(1:ncol(bbc), function(x)
#  trimws(paste(bbc[,x],collapse=" "),"right"))
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
bbc <- readLines("BBCnews.txt", encoding = "UTF-8")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- readLines("BBCnews.txt", encoding = "UTF-8")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
inspect(bbc)
inspect(docs)
bbc <- readLines("BBCnews.txt")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
inspect(docs)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs1 <- tm_map(docs, toSpace, "-")
docs2 <- tm_map(docs1, removeNumbers)
docs3 <- tm_map(docs2, removePunctuation)
docs4 <- tm_map(docs3, content_transformer(tolower))
docs5 <- tm_map(docs4, removeWords, stopwords("english"))
docs6 <- tm_map(docs5, stripWhitespace)
docs7 <- tm_map(docs6, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
wordcloud2(wf)
docs1 <- tm_map(docs, toSpace, "-")
docs2 <- tm_map(docs1, removeNumbers)
docs3 <- tm_map(docs2, removePunctuation)
docs4 <- tm_map(docs3, content_transformer(tolower))
docs5 <- tm_map(docs4, removeWords, stopwords("english"))
docs6 <- tm_map(docs5, stripWhitespace)
docs7 <- tm_map(docs6, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- readLines("BBCnews.txt")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc <- readLines("BBCnews.txt")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
inspect(docs)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
inspect(docs)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Test1")
library(tidyverse)
library(tm)
library(SnowballC)
library(textstem)
library(wordcloud2)
bbc <- readLines("BBCnews.txt")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
setwd("G:/My Drive/Master-Data-Science/Semester_2/Unstructured Data/Test1")
library(tidyverse)
library(tm)
library(SnowballC)
library(textstem)
library(wordcloud2)
bbc <- readLines("BBCnews.txt", encoding = "UTF-8")
mytext<-VectorSource(bbc)
docs<-Corpus(mytext)
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
wordcloud2(wf)
ggplot(wf,aes(x=TERM,y=FREQ)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45,hjust=1))
bbc = readLines("BBCnews.txt", encoding = "UTF-8")
docs = Corpus(VectorSource(bbc))
docs
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc = readLines("BBCnews.txt", encoding = "UTF-8")
docs = Corpus(VectorSource(bbc))
docs
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, lemmatize_strings)
dtm<-DocumentTermMatrix(docs)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
bbc = readLines("BBCnews.txt")
docs = Corpus(VectorSource(bbc))
docs
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
bbc = readLines("BBCnews.txt")
docs = Corpus(VectorSource(bbc))
docs
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs1 <- tm_map(docs, toSpace, "-")
docs2 <- tm_map(docs1, removeNumbers)
docs3 <- tm_map(docs2, removePunctuation)
docs4 <- tm_map(docs3, content_transformer(tolower))
docs5 <- tm_map(docs4, removeWords, stopwords("english"))
docs6 <- tm_map(docs5, stripWhitespace)
docs7 <- tm_map(docs6, lemmatize_strings)
dtm<-DocumentTermMatrix(docs7)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
docs3 <- tm_map(docs2, removePunctuation)
docs4 <- tm_map(docs3, content_transformer(tolower))
docs5 <- tm_map(docs4, removeWords, stopwords("english"))
docs6 <- tm_map(docs5, stripWhitespace)
docs7 <- tm_map(docs6, lemmatize_strings)
dtm<-DocumentTermMatrix(docs7)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
ord
freq
bbc = readLines("BBCnews.txt")
docs = Corpus(VectorSource(bbc))
docs
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ", x))
})
docs1 <- tm_map(docs, toSpace, "-")
docs2 <- tm_map(docs1, removeNumbers)
docs3 <- tm_map(docs2, removePunctuation)
docs4 <- tm_map(docs3, content_transformer(tolower))
docs6 <- tm_map(docs5, stripWhitespace)
docs7 <- tm_map(docs6, lemmatize_strings)
dtm<-DocumentTermMatrix(docs7)
inspect(dtm)
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=T)
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
# Sort the wf dataframe by FREQ
wf<-wf[order(wf$FREQ,decreasing=T),] %>%
filter(FREQ >= 5)
wf
inspect(docs)
inspect(docs7)
inspect(docs)
inspect(docs6)
inspect(docs5)
inspect(docs3)
inspect(docs2)
inspect(docs1)
inspect(docs)
bbc = readLines("BBCnews.txt")
docs = Corpus(VectorSource(bbc))
inspect(docs)
?readLines
bbc = readLines("BBCnews.txt", encoding = "UTF-8")
docs
inspect(docs)
